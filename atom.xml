<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://wangfengyi0228.github.io</id>
    <title>one in a million</title>
    <updated>2021-11-12T06:08:38.454Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://wangfengyi0228.github.io"/>
    <link rel="self" href="https://wangfengyi0228.github.io/atom.xml"/>
    <subtitle>我在你的心里，有没有一点特别</subtitle>
    <logo>https://wangfengyi0228.github.io/images/avatar.png</logo>
    <icon>https://wangfengyi0228.github.io/favicon.ico</icon>
    <rights>All rights reserved 2021, one in a million</rights>
    <entry>
        <title type="html"><![CDATA[OD | YOLO]]></title>
        <id>https://wangfengyi0228.github.io/post/od-or-yolo/</id>
        <link href="https://wangfengyi0228.github.io/post/od-or-yolo/">
        </link>
        <updated>2021-11-12T04:47:44.000Z</updated>
        <content type="html"><![CDATA[<p>参考文献</p>
<ul>
<li>[1]Redmon J , Divvala S , Girshick R , et al. You Only Look Once: Unified, Real-Time Object Detection[C]// Computer Vision &amp; Pattern Recognition. IEEE, 2016.</li>
<li>[2]Redmon J , Farhadi A . YOLO9000: Better, Faster, Stronger[C]// IEEE Conference on Computer Vision &amp; Pattern Recognition. IEEE, 2017:6517-6525.</li>
<li>[3]Redmon J , Farhadi A . YOLOv3: An Incremental Improvement[J]. arXiv e-prints, 2018.</li>
<li>[4]Bochkovskiy A , Wang C Y , Liao H Y M . YOLOv4: Optimal Speed and Accuracy of Object Detection[J]. 2020.</li>
</ul>
<h1 id="1yolov1">1.YOLOv1</h1>
<p><strong>1.1 概述</strong><br>
人类的视觉系统快而准确，只要看一眼图像就能知道图像中有什么物体，以及它们之间的相互关系。YOLO（You Only Look Once: Unified, Real-Time Object Detection）正是为了能够进行这样快速、准确的目标检测而诞生的，它创造性地将边界框和类别两个过程合二为一。与基于滑动窗口和region proposal的R-CNN、Fast R-CNN等算法不同，<code>YOLO训练和检测均是在一个单独网络中进行，没有求取region proposal的过程，而是将目标检测作为回归问题求解，基于一个单独的end-to-end网络，完成从原始图像的输入到物体位置和类别的输出。</code><br>
YOLO首先将图片分成<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>S</mi><mo>∗</mo><mi>S</mi></mrow><annotation encoding="application/x-tex">S*S</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.05764em;">S</span></span></span></span>个大小相同的grid，然后让每个grid预测出B个bounding boxes，每个边界框预测一个物体的中心位置(x,y)、高(h)、宽(w)，以及这次预测的置信度，每个grid识别出一种物体。若分类器可以识别C种不同的物体，那么预测结果总共有<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>S</mi><mo>∗</mo><mi>S</mi><mo>∗</mo><mo>(</mo><mi>B</mi><mo>∗</mo><mn>5</mn><mo>+</mo><mi>C</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">S*S*(B*5+C)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">5</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="mclose">)</span></span></span></span>个张量。<br>
<img src="https://wangfengyi0228.github.io/post-images/1636695019326.png" alt="" loading="lazy"><br>
上图展示了图像的分割过程，以及每一个grid预测出的bounding boxes的情况。YOLO的网络结构包括24个卷积层和2个全连接层，其中，卷积层用来提取图像特征，全连接层用来预测图像位置和类别概率值。YOLO网络借鉴了GoogLeNet分类网络结构。不同的是，YOLO未使用inception module，而是使用1x1卷积层（此处1x1卷积层的存在是为了跨通道信息整合）+3x3卷积层简单替代。<br>
<img src="https://wangfengyi0228.github.io/post-images/1636695041225.png" alt="" loading="lazy"><br>
<img src="https://wangfengyi0228.github.io/post-images/1636695047630.png" alt="" loading="lazy"><br>
第一项是物体中心坐标的损失函数，使用的是简单的均方误差；第二项是高和宽的损失函数，使用了平方根的均方误差，是为了使模型对较大的和较小的物体同样敏感，反映出大box中的小偏差比小box中的小偏差更重要。第三项是边框内有对象时的置信度误差，第四项是边框内无对象时的置信度误差，第五项是对象分类误差。<br>
在计算IOU误差时，包含物体的grid与不包含物体的grid，二者的IOU误差对网络loss的贡献值是不同的。两者若采用相同的权值，那么不包含物体的格子的confidence值近似为0，变相放大了包含物体的grid的confidence误差在计算网络参数梯度时的影响，可能会导致模型不稳定。为解决这一问题，YOLO的做法是<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>s</mi><mi>e</mi><mi>t</mi><msub><mi>λ</mi><mrow><mi>c</mi><mi>o</mi><mi>o</mi><mi>r</mi><mi>d</mi></mrow></msub><mo>=</mo><mn>5</mn><mi>a</mi><mi>n</mi><mi>d</mi><msub><mi>λ</mi><mrow><mi>n</mi><mi>o</mi><mi>o</mi><mi>b</mi><mi>j</mi></mrow></msub><mo>=</mo><mn>5</mn></mrow><annotation encoding="application/x-tex">set \lambda _{coord}=5 and \lambda _{noobj}=5</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord mathdefault">s</span><span class="mord mathdefault">e</span><span class="mord mathdefault">t</span><span class="mord"><span class="mord mathdefault">λ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">c</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight" style="margin-right:0.02778em;">r</span><span class="mord mathdefault mtight">d</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.980548em;vertical-align:-0.286108em;"></span><span class="mord">5</span><span class="mord mathdefault">a</span><span class="mord mathdefault">n</span><span class="mord mathdefault">d</span><span class="mord"><span class="mord mathdefault">λ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">n</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">b</span><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">5</span></span></span></span>来进行修正。<br>
<strong>1.2 YOLOv1的优点</strong><br>
YOLO将识别与定位合二为一，结构简洁，检测速度快。相对于R-CNN系列, YOLO的整个流程中都能看到整张图像的信息，因此它在检测物体时能很好的利用上下文信息，从而不容易在背景上预测出错误的物体信息。同时YOLO可以学习到高度泛化的特征，能将一个域上学到的特征迁移到不同但相关的域上，如在自然图像上做训练的YOLO，在艺术图片上可以得到较好的测试结果。</p>
<p><strong>1.3 YOLOv1的缺点</strong><br>
YOLOv1中一个grid只能识别出一种物体，且bounding box的预测准确度不如R-CNN等Region-based方法，其对小物体的检测，尤其是对密集的小物体检测效果较差。</p>
<h1 id="2yolov2">2.YOLOv2</h1>
<p>作者首先在YOLOv1的基础上提出了改进的YOLOv2，然后提出了一种检测与分类联合训练的方法，使用这种联合训练方法在COCO检测数据集和ImageNet分类数据集上训练出了YOLO9000模型，其可以检测超过9000多类物体。<br>
<strong>2.1 YOLOv2的改进策略</strong><br>
YOLOv1虽然检测速度很快，但是在检测精度上却不如R-CNN系检测方法，YOLOv1在物体定位方面不够准确，并且召回率较低。YOLOv2共提出了几种改进策略来提升YOLO模型的定位准确度和召回率，从而提高mAP，YOLOv2在改进中遵循一个原则：保持检测速度，这也是YOLO模型的一大优势。</p>
<ul>
<li>
<p>(1)Batch Normalization<br>
Batch Normalization有助于规范化模型，可以提升模型的收敛速度，而且可以起到一定的正则化效果，降低模型的过拟合。<code>在YOLOv2中，每个卷积层后面都添加了Batch Normalization层，并且不再使用droput。</code>使用Batch Normalization后，YOLOv2的mAP提升了2.4%。</p>
</li>
<li>
<p>(2)High Resolution Classifier<br>
目前大部分的检测模型都会在先在ImageNet分类数据集上预训练模型的主体部分，大多数分类器操作的输入图像小于256×256，分辨率相对较低，不利于检测模型。所以YOLOv1在采用224×224分类模型预训练后，将分辨率增加至448×448。但是直接切换分辨率，检测模型可能难以快速适应高分辨率。<code>所以YOLOv2增加了在ImageNet数据集上使用448×448的分辨率对分类网络进行10个epoch的精细调整，这给了网络时间来调整它的过滤器，以更好地工作在更高分辨率的输出，</code>然后在检测时对对得到的网络进行微调，可以使得模型适用高分辨率输入。使用高分辨率分类器后，YOLOv2的mAP提升了约4%。</p>
</li>
<li>
<p>(3)Convolutional With Anchor Boxes<br>
YOLOv1识别精度低的其中一个原因就是边界框的宽与高的确定存在问题。YOLOv1最后采用的是全连接层直接对边界框进行预测，其中边界框的宽与高是相对整张图片大小的，而由于各个图片中存在不同尺度和长宽比的物体，因此YOLOv1在训练过程中学习适应不同物体的形状是比较困难的，这也导致YOLOv1在精确定位方面表现较差。YOLOv2借鉴了Faster R-CNN中RPN网络的先验框策略。<code>RPN对CNN特征提取器得到的特征图进行卷积来预测每个位置的边界框以及置信度，并且各个位置设置不同尺度和比例的先验框，采用先验框使得模型更容易学习。</code><br>
所以YOLOv2移除了YOLOv1中的全连接层而采用了卷积和anchor boxes来预测边界框。为了使检测所用的特征图分辨率更高，移除其中的一个pool层。在检测模型中，YOLOv2不是采448×448图片作为输入，而是采用416×416大小。因为YOLOv2模型下采样的步长为32，对于416×416大小的图片，最终得到的特征图大小为13×13，维度是奇数，这样特征图恰好只有一个中心位置。对于一些大物体，它们中心点往往落入图片中心位置，此时使用特征图的一个中心点去预测这些物体的边界框相对容易些。<br>
使用anchor boxes之后，YOLOv2的mAP有稍微下降，但召回率有所上升。</p>
</li>
<li>
<p>(4)Dimension Clusters<br>
在Faster R-CNN和SSD中，先验框的长和宽是手动设定的，带有一定的主观性。<code>如果选取的先验框维度比较合适，那么模型更容易学习，从而做出更好的预测。因此，YOLOv2采用k-means聚类方法对训练集中的边界框做了聚类分析。</code>因为设置先验框的主要目的是为了使得预测框与ground truth的IOU更好，所以聚类分析时选用box与聚类中心box之间的IOU值作为距离指标：<br>
<img src="https://wangfengyi0228.github.io/post-images/1636695702119.png" alt="" loading="lazy"></p>
</li>
<li>
<p>(5) New Network：Darknet-19<br>
YOLOv2采用了一个新的基础模型（特征提取器），称为Darknet-19，包括19个卷积层和5个maxpooling层，如Table 6所示。Darknet-19与VGG16模型设计原则是一致的，主要采用3<em>3卷积，采用2</em>2的maxpooling层之后，特征图维度降低2倍，而同时将特征图的channles增加两倍。与NIN(Network in Network)类似，Darknet-19最终采用global avgpooling做预测，并且在3<em>3卷积之间使用1</em>1卷积来压缩特征图channles以降低模型计算量和参数。Darknet-19每个卷积层后面同样使用了batch norm层以加快收敛速度，降低模型过拟合。在ImageNet分类数据集上，Darknet-19的top-1准确度为72.9%，top-5准确度为91.2%，但是模型参数相对小一些。使用Darknet-19之后，YOLOv2的mAP值没有显著提升，但是计算量却可以减少约33%。<br>
<img src="https://wangfengyi0228.github.io/post-images/1636695596885.png" alt="" loading="lazy"></p>
</li>
<li>
<p>(6)Fine-Grained Features<br>
YOLOv2的输入图片大小为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>416</mn><mo>∗</mo><mn>416</mn></mrow><annotation encoding="application/x-tex">416*416</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">4</span><span class="mord">1</span><span class="mord">6</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">4</span><span class="mord">1</span><span class="mord">6</span></span></span></span>，经过5次maxpooling之后得到<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>13</mn><mo>∗</mo><mn>13</mn></mrow><annotation encoding="application/x-tex">13*13</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord">3</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord">3</span></span></span></span>大小的特征图，并以此特征图采用卷积做预测。<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>13</mn><mo>∗</mo><mn>13</mn></mrow><annotation encoding="application/x-tex">13*13</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord">3</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord">3</span></span></span></span>大小的特征图对检测大物体是足够了，但是对于小物体还需要更精细的特征图（Fine-Grained Features）。因此SSD使用了多尺度的特征图来分别检测不同大小的物体，前面更精细的特征图可以用来预测小物体。YOLOv2提出了一种passthrough层来利用更精细的特征图。YOLOv2所利用的Fine-Grained Features是<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>26</mn><mo>∗</mo><mn>26</mn></mrow><annotation encoding="application/x-tex">26*26</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">2</span><span class="mord">6</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">2</span><span class="mord">6</span></span></span></span>大小的特征图（最后一个maxpooling层的输入），对于Darknet-19模型来说就是大小为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>26</mn><mo>∗</mo><mn>26</mn><mo>∗</mo><mn>512</mn></mrow><annotation encoding="application/x-tex">26*26*512</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">2</span><span class="mord">6</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">2</span><span class="mord">6</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">5</span><span class="mord">1</span><span class="mord">2</span></span></span></span>的特征图。passthrough层与ResNet网络的shortcut类似，以前面更高分辨率的特征图为输入，然后将其连接到后面的低分辨率特征图上。前面的特征图维度是后面的特征图的2倍，passthrough层抽取前面层的每个<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>2</mn><mo>∗</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">2*2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">2</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">2</span></span></span></span>的局部区域，然后将其转化为channel维度，对于<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>26</mn><mo>∗</mo><mn>26</mn><mo>∗</mo><mn>512</mn></mrow><annotation encoding="application/x-tex">26*26*512</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">2</span><span class="mord">6</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">2</span><span class="mord">6</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">5</span><span class="mord">1</span><span class="mord">2</span></span></span></span>的特征图，经passthrough层处理之后就变成了<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>13</mn><mo>∗</mo><mn>13</mn><mo>∗</mo><mn>2048</mn></mrow><annotation encoding="application/x-tex">13*13*2048</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord">3</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord">3</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">2</span><span class="mord">0</span><span class="mord">4</span><span class="mord">8</span></span></span></span>的新特征图（特征图大小降低4倍，而channles增加4倍，图6为一个实例），这样就可以与后面的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>13</mn><mo>∗</mo><mn>13</mn><mo>∗</mo><mn>1024</mn></mrow><annotation encoding="application/x-tex">13*13*1024</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord">3</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord">3</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord">0</span><span class="mord">2</span><span class="mord">4</span></span></span></span>特征图连接在一起形成<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>13</mn><mo>∗</mo><mn>13</mn><mo>∗</mo><mn>3072</mn></mrow><annotation encoding="application/x-tex">13*13*3072</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord">3</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord">3</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">3</span><span class="mord">0</span><span class="mord">7</span><span class="mord">2</span></span></span></span>的特征图，然后在此特征图基础上卷积做预测。在YOLO的C源码中，passthrough层称为reorg layer。在TensorFlow中，可以使用tf.extract_image_patches或者tf.space_to_depth来实现passthrough层。<br>
#3.YOLOv3<br>
YOLOv3借鉴了一些好的方案融合到YOLO里面，在保持速度优势的前提下，提升了预测精度，尤其是加强了对小物体的识别能力。</p>
</li>
<li>
<p><strong>DBL：</strong> 代码中的Darknetconv2d_BN_Leaky，是yolo_v3的基本组件,就是卷积+BN+Leaky relu。对于v3来说，BN和leaky relu已经是和卷积层不可分离的部分了(最后一层卷积除外)，共同构成了最小组件。</p>
</li>
<li>
<p>**resn： **n代表数字，有res1，res2, … ,res8等等，表示这个res_block里含有多少个res_unit。这是yolo_v3的大组件，yolo_v3开始借鉴了ResNet的残差结构，使用这种结构可以让网络结构更深(从v2的darknet-19上升到v3的darknet-53，前者没有残差结构)。res_block的基本组件也是DBL。</p>
</li>
<li>
<p>**concat： *<em>张量拼接。将darknet中间层和后面的某一层的上采样进行拼接。拼接的操作和残差层add的操作是不一样的，拼接会扩充张量的维度，而add只是直接相加不会导致张量维度的改变。<br>
<strong>(1)backbone：darknet-53</strong><br>
为了达到更好的分类效果，作者自己设计训练了darknet-53。作者在ImageNet上实验发现这个darknet-53，的确很强，相对于ResNet-152和ResNet-101，darknet-53不仅在分类精度上差不多，计算速度还比ResNet-152和ResNet-101快得多，网络层数也较少。<code>YOLOv3使用了darknet-53的前面的52层（没有全连接层），yolo_v3这个网络是一个全卷积网络，大量使用残差的跳层连接，并且为了降低池化带来的梯度负面效果，作者直接摒弃了Pooling，用conv的stride来实现降采样。</code>在这个网络结构中，使用的是步长为2的卷积来进行降采样。<br>
为了加强算法对小目标检测的精确度，YOLO v3中采用类似FPN的upsample和融合做法（最后融合了3个scale，其他两个scale的大小分别是26×26和52×52），在多个scale的feature map上做检测。<br>
<strong>(2)Output</strong><br>
yolo v3输出了3个不同尺度的feature map，即predictions across scales，这个借鉴了FPN(feature pyramid networks)<code>采用多尺度来对不同size的目标进行检测，越精细的grid cell就可以检测出越精细的物体。</code>3个尺度的feature mapy1,y2和y3的深度都是255，边长的规律是13:26:52。对于COCO类别而言，有80个种类，所以每个box应该对每个种类都输出一个概率。yolo v3设定的是每个网格单元预测3个box，所以每个box需要有(x, y, w, h, confidence)五个基本参数，然后还要有80个类别的概率，3</em>(5 + 80) = 255。而yolo v1的输出张量是7x7x30，只能识别20类物体，显然YOLOv3改进了许多。<br>
<strong>(3)Bounding Box</strong><br>
YOLOv3的Bounding Box由YOLOv2又做出了更好的改进。在YOLOv2和YOLOv3中，都对图像中的object采用了k-means聚类。 feature map中的每一个cell都会预测3个边界框（bounding box），每个bounding box都会预测三个东西：每个框的位置（4个值，中心坐标tx和ty，框的高度bh和宽度bw）、一个objectness prediction 、N个类别，coco数据集80类，voc20类。三次检测，每次对应的感受野不同，适合检测的物体大小不同。<br>
<img src="https://wangfengyi0228.github.io/post-images/1636695931976.png" alt="" loading="lazy"><br>
<strong>(4)LOSS Function</strong><br>
YOLOv3重要改变之一：No more softmaxing the classes。YOLO v3对图像中检测到的对象执行多标签分类。logistic回归用于对anchor包围的部分进行一个目标性评分(objectness score)，即这块位置是目标的可能性有多大。这一步是在predict之前进行的，可以去掉不必要的anchor，减少计算量。不同于faster R-CNN的是，yolo_v3只会对1个prior进行操作，也就是那个最佳prior。而logistic回归就是用来从9个anchor priors中找到objectness score(目标存在可能性得分)最高的那一个。logistic回归就是用曲线对prior相对于 objectness score映射关系的线性建模。</p>
</li>
</ul>
<h1 id="4yolov4">4.YOLOv4</h1>
<p><strong>(1)Bag of freebies</strong><br>
在目标检测中Bag of freebies是指：用一些比较有用的训练技巧来训练模型，从而使目标检测器在不增加推理损耗的情况下达到更好的精度。目标检测经常采用这种方法，并符合这个定义的就是数据增强。<code>数据增强的目的是：增加输入图像的可变性，从而使设计的目标检测模型对不同环境的图片具有较高的鲁棒性。</code><br>
其他的一些Bag of freebies方法是专门解决数据集中的语义分布偏差。在处理语义困扰的问题上，有一个很重要的问题是不同类别之间的数据不平衡，而two-stage 检测器处理这个问题通常是通过hard negative example mining或online hard example mining 。 最后一个bag of freebies是objective function of Bounding Box (BBox) 回归。检测器通常使用MSE对BBOX的中心点和宽高进行回归，至于anchor-based方法，它是为了估算出对应的偏移量。但是，要直接估计BBOX的点坐标值，是要将这些点作为独立变量，但实际上未考虑对象本身的完整性。为了使这一问题得到更好的处理，一些研究人员最近提出的IoU损失，同时考虑预测的BBox面积和ground truth BBox面积覆盖。</p>
<p><strong>(2)Bag of specials</strong><br>
Bag of specials是指一些plugin modules（例如特征增强模型，或者一些后处理），这部分增加的计算量很少，但是能有效地增加物体检测的准确率，将这部分称之为Bag of specials。这部分插件模块能够增强网络模型的一些属性，例如增大感受野（ASFF，ASPP，RFB这些模块）、引入注意力机制（spatial attention和channel attention）、增加特征集成能力。后处理算法是指用一些算法来筛选模型预测出来的结果，后处理方法常用的是NMS，它可以用于过滤那些预测错误的BBoxes，并只保留较高的候选BBoxes。</p>
<p><strong>(3)额外改进</strong><br>
使用一种新的数据增强方法Mosaic和Self-Adversarial Training (SAT)，前者把四张图拼成一张图来训练，变相的等价于增大了mini-batch；后者是在一张图上，让神经网络反向更新图像，对图像做改变扰动，然后在这个图像上训练。<br>
<img src="https://wangfengyi0228.github.io/post-images/1636696843641.png" alt="" loading="lazy"><br>
跨最小批的归一化（Cross mini-batch Normal），在CBN的基础上修改的SAM，从spatial-wise attention修改为point-wise attention：<br>
<img src="https://wangfengyi0228.github.io/post-images/1636696855738.png" alt="" loading="lazy"><br>
修改的PAN，把通道从相加（add）改变为concat：<br>
<img src="https://wangfengyi0228.github.io/post-images/1636696863193.png" alt="" loading="lazy"><br>
<strong>(4)YOLOv4的组成</strong><br>
Backbone: CSPDarknet53<br>
Neck: SPP , PAN<br>
Head: YOLOv3</p>
<h1 id="5总结">5.总结</h1>
<p><strong>(1)YOLOv1</strong></p>
<ul>
<li>训练和检测均是在一个单独网络中进行，没有求取region proposal的过程。</li>
<li>将目标检测作为回归问题求解，基于一个单独的end-to-end网络。</li>
<li>损失函数中的几个项与输出向量中的内容相对应。</li>
<li>YOLOv1检测速度快，但在检测小物体以及密集物体方面存在欠缺。<br>
<strong>(2)YOLOv2</strong></li>
<li>YOLOv2在保证检测速度的基础上提出了几种改进策略来提升YOLO模型的定位准确度和召回率。</li>
<li>网络结构中每个卷积层后面都添加了Batch Normalization层，并且使用高了分辨率分类器。</li>
<li>YOLOv2采用k-means聚类方法对训练集中的边界框做了聚类分析，使得选取的先验框维度更合适，模型更容易学习，从而做出更好的预测。<br>
<strong>(3)YOLOv3</strong></li>
<li>YOLOv3借鉴了一些好的方案融合到YOLO里面，在保持速度优势的前提下，提升了预测精度，尤其是加强了对小物体的识别能力。</li>
<li>Darknetconv2d_BN_Leaky是yolo_v3的基本组件,就是卷积+BN+Leaky relu。</li>
<li>YOLOv3开始借鉴了ResNet的残差结构，使用这种结构可以让网络结构更深。</li>
<li>yolo v3输出了3个不同尺度的feature map，采用多尺度来对不同size的目标进行检测，越精细的grid cell就可以检测出越精细的物体。</li>
<li>YOLOv3的Bounding Box由YOLOv2又做出了更好的改进。在YOLOv2和YOLOv3中，都对图像中的object采用了k-means聚类。 feature map中的每一个cell都会预测3个边界框。三次检测，每次对应的感受野不同，适合检测的物体大小不同。<br>
<strong>(4)YOLOv4</strong></li>
<li>YOLOv4研究设计了一个简单且高效的目标检测算法，该算法降低了训练门槛，使得普通人员在拥有一块1080TI或者2080TI的情况下就能够训练一个super fast and accurate 的目标检测器。</li>
<li>在训练过程中，验证了最新的Bag-of-Freebies和Bag-of-Specials对Yolo-V4的影响。</li>
<li>简化以及优化了一些最新提出的算法，包括（CBN，PAN，SAM），从而使Yolo-V4能够在一块GPU上就可以训练起来。</li>
</ul>
<p>YOLOv5初步移植到手机后的效果图：<br>
<img src="https://wangfengyi0228.github.io/post-images/1636696976292.png" alt="" loading="lazy"></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Qt | 智能巡检机器人项目]]></title>
        <id>https://wangfengyi0228.github.io/post/qt-or-zhi-neng-xun-jian-ji-qi-ren-xiang-mu/</id>
        <link href="https://wangfengyi0228.github.io/post/qt-or-zhi-neng-xun-jian-ji-qi-ren-xiang-mu/">
        </link>
        <updated>2021-08-28T03:38:37.000Z</updated>
        <content type="html"><![CDATA[<blockquote>
<p>notes:该机器人使用卫星进行经纬度定位，适用于开阔地带，初始化搜星数达到19以上为佳，距离精度精确到小数点后16位（以米为单位）<br>
<strong>项目总体框图：</strong><br>
<img src="https://wangfengyi0228.github.io/post-images/1636688828688.png" alt="" loading="lazy"><br>
<strong>逻辑流程图：</strong><br>
<img src="https://wangfengyi0228.github.io/post-images/1636691559203.png" alt="" loading="lazy"><br>
<strong>ChasisSender</strong><br>
ChasisSender构造即启动两个串口（Imu串口和Cmd串口），Imu串口负责接收Imu数据和定位数据，将数据解析后发送到LtkjCenter。Cmd串口负责发送控制指令（速度和转角）到底盘控制器，自定义格式为“$cmd,转角,速度\r\n”。Cmd串口数据的发送在接收到LtkjCenter的control_msg信号后，开始发送，接收到control_msg.cmd_stop信号后关闭cmd串口，停止发送。</p>
</blockquote>
<ul>
<li>Signal:Chasis_state_signal(ChasisState) 向LtkjCenter发送底盘状态</li>
<li>Signal:Car_position_signal(Position) 向LtkjCenter发送车辆位置</li>
<li>Slot:ProcessImuMsg_slot() 接收Imu串口消息</li>
</ul>
<pre><code>void ChasisSender::ProcessImuMsg_slot()
{
    if(!imu_serial-&gt;canReadLine())   return ;
    QByteArray readData = imu_serial-&gt;readAll();
    if(readData.isEmpty())  return ;

    if(readData[0] == '$')
    {
        QString ImuData_str = QString(readData);
        QStringList tmpImuData_Lis = ImuData_str.split(&quot;,&quot;);

        if (tmpImuData_Lis.count() == 24)
        {
            QString imuYawangle_str = tmpImuData_Lis[3];
            QString imuLatitude_str = tmpImuData_Lis[12];
            QString imuLongitude_str = tmpImuData_Lis[13];

            Position tmp_position(0, 0, 0);
            tmp_position.x = imuLongitude_str.toDouble();
            tmp_position.y = imuLatitude_str.toDouble();
            tmp_position.theta = imuYawangle_str.toDouble();

            WGS2CART(car_position_,tmp_position);
            car_position_.x -= Origin_position_.x;
            car_position_.y -= Origin_position_.y;

            qDebug() &lt;&lt; &quot;x: &quot; &lt;&lt; car_position_.x &lt;&lt; &quot; y: &quot; &lt;&lt; car_position_.y &lt;&lt; &quot; z: &quot; &lt;&lt; car_position_.theta;

            chasis_state_.imu_state_ = 1;
            emit Chasis_state_signal(chasis_state_); 
            emit Car_position_signal(car_position_);  
       }
    }
    imu_serial-&gt;clear();
}

•Slot:ReceiveCmdMsg_slot(control_msg) 接收LtkjCenter控制指令
void ChasisSender::ReceiveCmdMsg_slot(control_msg msg)  
{
    if(!msg.cmd_stop)
    {
        if(cmd_serial-&gt;isOpen())
        {
            cmd_angle = msg.cmd_angle;
            cmd_speed = msg.cmd_speed;
            SendCmdSerialData();
        }
        else
        {
            OpenCmdSerialport();
        }
    }
    else
    {
        if(cmd_serial-&gt;isOpen())
        {
            CloseCmdSerialport();
        }
    }
}
</code></pre>
<p><strong>UdpReceiver</strong><br>
UdpReceiver构造即启动udp接收，具体绑定的IP和端口需要在构造函数中修改，这里不再提供修改的接口。接收的槽函数已经写好，解析需要根据协议来定，这个留着后期再加。测试阶段只保证实现功能。在udp接收槽接收到消息之后，进行解析，发送任务指令到LtkjCenter。</p>
<ul>
<li>Signal:CommandSingal(task_msg) 向LtkjCenter发送任务命令</li>
<li>Slot:Udp_receiver_slot() 接收udp消息并进行解析</li>
<li>Slot:Udp_position_slot(Position); 接收LtkjCenter车辆位置信息</li>
<li>Slot:Send_Timer_slot() 向LtkjCenter发送心跳</li>
</ul>
<pre><code>void UdpReceiver::Udp_receiver_slot() 
{
    while (udp_receiver_-&gt;hasPendingDatagrams()) 
    {
        QByteArray datagram;
        datagram.resize(udp_receiver_-&gt;pendingDatagramSize( ));
        udp_receiver_-&gt;readDatagram (datagram.data( ), datagram.size());

        if(datagram[0] == '$')
        {
            QString udp_str = QString(datagram);
            QStringList udp_str_list = udp_str.split(&quot;,&quot;);
            if(udp_str_list.count() == 4)
            {
                action = udp_str_list[1].toInt();
                end_pos_.setX(udp_str_list[2].toDouble());
                end_pos_.setY(udp_str_list[3].toDouble());
            }
        }
        // qDebug() &lt;&lt; datagram;
        task_msg_.state = action;
        task_msg_.point_target = end_pos_;
        emit CommandSingal(task_msg_);
    }
}
</code></pre>
<p><strong>LtkjCenter</strong><br>
LtkjCenter包含了LtkjMap和LtkjControl两个类。<br>
逻辑见流程图</p>
<ul>
<li>Signal:Chasis_cmd_signal(control_msg) 向ChasisSender发送控制指令，转角和速度以及停止信号</li>
<li>Slot:Command_slot(QString cmd) 接收UdpReceiver任务消息</li>
</ul>
<pre><code>void LtkjCenter::Command_slot(task_msg cmd)
{
    qDebug() &lt;&lt; &quot;rece from udp: &quot; &lt;&lt; cmd.state &lt;&lt; &quot; and &quot; &lt;&lt; cmd.point_target;
    if(cmd.state == 0)
    {
        if(track_timer-&gt;isActive())
        {
            track_timer-&gt;stop();
        }
        SetReady();
    }
    else if(cmd.state == 1)
    {
        end_position_.x = cmd.point_target.x();
        end_position_.y = cmd.point_target.y();
        end_position_.theta = 0;

        if(map_path_state_.map_state_ == 1 &amp;&amp; chasis_state_.imu_state_ == 1)
        {
            Position2QPointF(car_point, car_position_);
            Position2QPointF(end_point, end_position_);
            map_.AStar(car_point, end_point);           
            UpdateMapState();
        }
        else
        {
            qDebug() &lt;&lt; &quot;Error: map_state = 0 !&quot;;
            SetReady();
            return ;
        }

        if(map_path_state_.path_state_ == 1)
        {
            if(!track_timer-&gt;isActive())
            {
                track_timer-&gt;start(100);
            }
            else
            {
                SetReady();
                track_timer-&gt;start(100);
                qDebug() &lt;&lt; &quot;Warning: New Task Failed, Track task is in Progress&quot;;
            }

        }
        else
        {
            qDebug() &lt;&lt; &quot;Error: map_path = 0!&quot;;
            return ;
        }
    }
}
</code></pre>
<ul>
<li>Slot:Chasis_state_slot(ChasisState) 接收ChasisSender底盘状态消息<pre><code></code></pre>
</li>
</ul>
<p>void LtkjCenter::Chasis_state_slot(ChasisState state)<br>
{<br>
chasis_state_.imu_state_ = state.imu_state_;<br>
}```</p>
<ul>
<li>Slot:Chasis_position_slot(Position) 接收ChasisSender车辆位置消息</li>
</ul>
<pre><code class="language-void">{
    car_position_.x = pos.x;
    car_position_.y = pos.y;
    car_position_.theta = pos.theta;

    emit Position_signal(car_position_);
}```
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Practice | Missing number]]></title>
        <id>https://wangfengyi0228.github.io/post/practice-or-missing-number/</id>
        <link href="https://wangfengyi0228.github.io/post/practice-or-missing-number/">
        </link>
        <updated>2021-04-26T04:40:22.000Z</updated>
        <content type="html"><![CDATA[<p>Given a positive integer n(n≤40), pick n-1 numbers randomly from 1 to n and concatenate them in random order as a string s, which means there is a missing number between 1 and n. Can you find the missing number?(Notice that in some cases the answer will not be unique, and in these cases you only need to find one valid answer.）</p>
<h2 id="sample-input-1">Sample Input 1:</h2>
<p>20<br>
81971112205101569183132414117</p>
<h2 id="sample-output-1">Sample Output 1:</h2>
<p>16</p>
<h1 id="solution">Solution:</h1>
<pre><code>#include &lt;bits/stdc++.h&gt;

using namespace std;

int len,n;
bool f[100];
string s;
int num[100];

void dfs(int k)
{
    if(k&gt;=len)
    {
        for(int i=1;i&lt;=n;i++)
        {
            if(f[i]==false)
            {
                printf(&quot;%d\n&quot;,i);
            }
        }
        return;
    }
    if(f[num[k]]==false)
    {
        f[num[k]]=true;
        dfs(k+1);
        f[num[k]]=false;
    }
    if(f[num[k]*10+num[k+1]]==false&amp;&amp;num[k]*10+num[k+1]&lt;=n)
    {
        f[num[k]*10+num[k+1]]=true;
        dfs(k+2);
        f[num[k]*10+num[k+1]]=false;
    }
}

int main()
{
    scanf(&quot;%d&quot;,&amp;n);
    cin&gt;&gt;s;
    for(int i=0;i&lt;s.size();i++)
    {
        num[i]=s[i]-'0';
    }
    len=s.size();
    dfs(0);
    return 0;
}
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Practice | Count number of binary strings without consecutive 1’s]]></title>
        <id>https://wangfengyi0228.github.io/post/practice-or-count-number-of-binary-strings-without-consecutive-1s/</id>
        <link href="https://wangfengyi0228.github.io/post/practice-or-count-number-of-binary-strings-without-consecutive-1s/">
        </link>
        <updated>2021-03-16T04:43:16.000Z</updated>
        <content type="html"><![CDATA[<p>Given a positive integer n(3≤n≤90), count all possible distinct binary strings of length n such that there are no consecutive 1's .</p>
<h2 id="sample-input-1">Sample Input 1:</h2>
<p>2</p>
<h2 id="sample-output-1">Sample Output 1:</h2>
<p>3 // The 3 strings are 00, 01, 10</p>
<h2 id="sample-input-2">Sample Input 2:</h2>
<p>3</p>
<h2 id="sample-output-2">Sample Output 2:</h2>
<p>5 // The 5 strings are 000, 001, 010, 100, 101</p>
<h1 id="solution">Solution:</h1>
<pre><code>#include &lt;bits/stdc++.h&gt;

using namespace std;

int pao(int n)
{
    if(n==1)
    {
        return 2;
    }
    else if(n==2)
    {
        return 3;
    }
    else
    {
        return pao(n-1)+pao(n-2);
    }
}

int main()
{
    int n;
    scanf(&quot;%d&quot;,&amp;n);
    int dp[100][2];
    dp[1][0]=1;
    dp[1][1]=1;
    for(int i=2;i&lt;=n;i++)
    {
        dp[i][0]=dp[i-1][0]+dp[i-1][1];
        dp[i][1]=dp[i-1][0];
    }
    printf(&quot;%d\n&quot;,dp[n][0]+dp[n][1]);
    printf(&quot;%d\n&quot;,pao(n));
    return 0;
}
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[OD | 术语名词]]></title>
        <id>https://wangfengyi0228.github.io/post/od-or-zhu-yu-ming-ci/</id>
        <link href="https://wangfengyi0228.github.io/post/od-or-zhu-yu-ming-ci/">
        </link>
        <updated>2020-11-30T04:45:25.000Z</updated>
        <content type="html"><![CDATA[<p><strong>1.学习率</strong><br>
学习率是在梯度下降的过程中更新权重时的超参数，当学习率过大的时候会导致模型难以收敛，过小的时候会收敛速度过慢，学习率是一个十分重要的超参数，合理的学习率才能让模型收敛到最小点而非局部最优点或鞍点。</p>
<p><strong>2.损失函数</strong><br>
损失函数是将随机事件或其有关随机变量的取值映射为非负实数，以表示该随机事件的“风险”或“损失”的函数。</p>
<p><strong>3.召回率</strong><br>
召回率是针对原来的样本而言的，它表示的是样本中的正例有多少被预测正确了。预测有两种可能，一种是把原来的正类预测成正类(TP)，另一种就是把原来的正类预测为负类(FN)。则</p>
<p><strong>4.交并比（IOU）</strong><br>
交并比时在目标检测任务中，计算两个边界框（定位的边界框和实际边界框）的交集和并集之比。一般约定,0.5 是阈值，用来判断预测的边界框是否正确，IOU越高，边界框越精确。</p>
<p><strong>5.非极大值抑制</strong><br>
算法可能对同一个对象做出多次检测，非极大值抑制这个方法可以确保算法对每个对象只检测一次。首先看检测概率最大的边界框，进行标记，之后非极大值抑制就会逐一审视剩下的矩形，所有和这个最大的边框有很高交并比、高度重叠的其他边界框，输出就会被抑制。</p>
<p><strong>6.mAP(mean Average Precision)</strong><br>
查准率（Precision）: TP/(TP + FP);查全率（即各类别 AP 的平均值Recall）: TP/(TP + FN)。二者绘制的曲线称为 P-R 曲线。AP是PR 曲线下面积。mAP即各类AP的平均值，用来衡量目标检测中的识别精度。</p>
<p><strong>7.感受野（Receptive Field）</strong><br>
在卷积神经网络中，感受野（Receptive Field）的定义是卷积神经网络每一层输出的特征图（feature map）上的像素点在输入图片上映射的区域大小，即特征图上的一个点对应输入图上的区域。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Practice | Root of AVL Tree]]></title>
        <id>https://wangfengyi0228.github.io/post/practice-or-root-of-avl-tree/</id>
        <link href="https://wangfengyi0228.github.io/post/practice-or-root-of-avl-tree/">
        </link>
        <updated>2019-11-19T04:36:26.000Z</updated>
        <content type="html"><![CDATA[<p>An AVL tree is a self-balancing binary search tree. In an AVL tree, the heights of the two child subtrees of any node differ by at most one; if at any time they differ by more than one, rebalancing is done to restore this property. Figures 1-4 illustrate the rotation rules.<br>
<img src="https://wangfengyi0228.github.io/post-images/1636692006948.png" alt="" loading="lazy"><br>
Now given a sequence of insertions, you are supposed to tell the root of the resulting AVL tree.</p>
<h1 id="input-specification">Input Specification:</h1>
<p>Each input file contains one test case. For each case, the first line contains a positive integer N (≤20) which is the total number of keys to be inserted. Then N distinct integer keys are given in the next line. All the numbers in a line are separated by a space.</p>
<h1 id="output-specification">Output Specification:</h1>
<p>For each test case, print the root of the resulting AVL tree in one line.</p>
<h2 id="sample-input-1">Sample Input 1:</h2>
<p>5<br>
88 70 61 96 120</p>
<h2 id="sample-output-1">Sample Output 1:</h2>
<p>70</p>
<h2 id="sample-input-2">Sample Input 2:</h2>
<p>7<br>
88 70 61 96 120 90 65</p>
<h2 id="sample-output-2">Sample Output 2:</h2>
<p>88</p>
<h1 id="solution">Solution:</h1>
<pre><code>#include &lt;bits/stdc++.h&gt;

using namespace std;

typedef struct node
{
    int data,height;
    node *lchild,*rchild;
}node;

int getheight(node* k)
{
    if(k==nullptr)
        return 0;
    else return k-&gt;height;
}

node* l_rotate(node* &amp;k1)
{
    node *k2=k1-&gt;rchild;

    k1-&gt;rchild=k2-&gt;lchild;
    k2-&gt;lchild=k1;
    k1-&gt;height=max(getheight(k1-&gt;lchild),getheight(k1-&gt;rchild))+1;
    k2-&gt;height=max(getheight(k2-&gt;lchild),getheight(k2-&gt;rchild))+1;
    return k2;
}

node* r_rotate(node* &amp;k1)
{
    node *k2=k1-&gt;lchild;

    k1-&gt;lchild=k2-&gt;rchild;
    k2-&gt;rchild=k1;
    k1-&gt;height=max(getheight(k1-&gt;rchild),getheight(k1-&gt;lchild))+1;
    k2-&gt;height=max(getheight(k2-&gt;rchild),getheight(k2-&gt;lchild))+1;
    return k2;
}

node* lr_rotate(node* &amp;k1)
{
    k1-&gt;lchild=l_rotate(k1-&gt;lchild);
    return r_rotate(k1);
}

node* rl_rotate(node* &amp;k1)
{
    k1-&gt;rchild=r_rotate(k1-&gt;rchild);
    return l_rotate(k1);
}

node* insertnode(int d,node* &amp;root)
{
    if(root==nullptr)
    {
        root=(node*)malloc(sizeof(node));
        root-&gt;data=d;
        root-&gt;lchild=nullptr;
        root-&gt;rchild=nullptr;
        root-&gt;height=0;
    }
    else if(d&lt;root-&gt;data)
    {
        root-&gt;lchild=insertnode(d,root-&gt;lchild);
        if(getheight(root-&gt;lchild)-getheight(root-&gt;rchild)==2)
        {
            if(d&lt;root-&gt;lchild-&gt;data)
            {
                root=r_rotate(root);
            }
            else
            {
                root=lr_rotate(root);
            }
        }
    }
    else if(d&gt;root-&gt;data)
    {
        root-&gt;rchild=insertnode(d,root-&gt;rchild);
        if(getheight(root-&gt;rchild)-getheight(root-&gt;lchild)==2)
        {
            if(d&gt;root-&gt;rchild-&gt;data)
            {
                root=l_rotate(root);
            }
            else
            {
                root=rl_rotate(root);
            }
        }
    }
    root-&gt;height=max(getheight(root-&gt;lchild),getheight(root-&gt;rchild))+1;
    return root;
}

int main()
{
    int n;
    cin&gt;&gt;n;
    node *root=nullptr;
    for(int i=0;i&lt;n;i++)
    {
        int tmp;
        scanf(&quot;%d&quot;,&amp;tmp);
        root=insertnode(tmp,root);
    }
    printf(&quot;%d&quot;,root-&gt;data);
    return 0;
}
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Welcome to My World]]></title>
        <id>https://wangfengyi0228.github.io/post/hello-gridea/</id>
        <link href="https://wangfengyi0228.github.io/post/hello-gridea/">
        </link>
        <updated>2000-02-28T14:45:00.000Z</updated>
        <content type="html"><![CDATA[<p>2000-02-28  Student ，  XMU</p>
<p>👏 欢迎来到我的blog ！持续建设ing<br>
✍️ 记录我的笔记、生活、心情... ...<br>
🤙 不要问我为什么图文不符，因为我愿意😋</p>
<p>🎓 想要了解更多或联系博主请查看关于<br>
🕙 想要按时间检索文章请查看归档<br>
📑 想要按tag检索文章请查看标签<br>
🔝右上角有导航栏哦</p>
<p>🌱 当然 wfy 还有很多不足，但请相信我会不停向前 🏃</p>
<p>😎 Enjoy~</p>
<p>2021.9.19 | 更新了标签检索功能<br>
2021.9.21 | 尝试添加评论功能失败，因为disqus被墙了😭，小王还在努力中<br>
2021.9.27 | 新增网站访客统计及访问量统计功能<br>
2021.9.28 | 新增文章阅读量统计功能<br>
2021.10.2 | 新增Katex渲染，可以用markdown写公式啦</p>
]]></content>
    </entry>
</feed>