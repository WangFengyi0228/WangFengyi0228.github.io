<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://wangfengyi0228.github.io</id>
    <title>one in a million</title>
    <updated>2021-11-13T07:02:18.708Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://wangfengyi0228.github.io"/>
    <link rel="self" href="https://wangfengyi0228.github.io/atom.xml"/>
    <subtitle>我在你的心里，有没有一点特别</subtitle>
    <logo>https://wangfengyi0228.github.io/images/avatar.png</logo>
    <icon>https://wangfengyi0228.github.io/favicon.ico</icon>
    <rights>All rights reserved 2021, one in a million</rights>
    <entry>
        <title type="html"><![CDATA[ML | 手推BP算法]]></title>
        <id>https://wangfengyi0228.github.io/post/ml-or-shou-tui-bp-suan-fa/</id>
        <link href="https://wangfengyi0228.github.io/post/ml-or-shou-tui-bp-suan-fa/">
        </link>
        <updated>2021-10-13T10:45:15.000Z</updated>
        <content type="html"><![CDATA[<p><img src="https://wangfengyi0228.github.io/post-images/1636714022094.jpg" alt="" loading="lazy"><br>
<img src="https://wangfengyi0228.github.io/post-images/1636714034678.png" alt="" loading="lazy"></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Movies | Carol]]></title>
        <id>https://wangfengyi0228.github.io/post/movies-or-carol/</id>
        <link href="https://wangfengyi0228.github.io/post/movies-or-carol/">
        </link>
        <updated>2021-10-03T06:59:29.000Z</updated>
        <content type="html"><![CDATA[<p>Some people change your life forever.<br>
<img src="https://wangfengyi0228.github.io/post-images/1636786833086.webp" alt="" loading="lazy"><br>
We like certain people. you like certain people, right？<br>
You don't like others.<br>
And you don't know why you're attracted to some people and not others，<br>
the only thing you know is you either are attracted or you're not attracted.<br>
It's like physics-bouncing off each other like pin balls.<br>
<img src="https://wangfengyi0228.github.io/post-images/1636786837190.webp" alt="" loading="lazy"><br>
Isn't that something other people let you know you have.<br>
And all you can do is...<br>
keep working.<br>
Use what feels right.<br>
Throw away the rest.<br>
<img src="https://wangfengyi0228.github.io/post-images/1636786841281.webp" alt="" loading="lazy"><br>
I drink to forget I gotta get up for work in the morning.<br>
You really ought to drink because you remember you have a job.<br>
Employment is a curse.<br>
<img src="https://wangfengyi0228.github.io/post-images/1636786845038.webp" alt="" loading="lazy"><br>
And that was it.<br>
For a while.<br>
And then it changed.<br>
It changes.<br>
Nobody's fault.<br>
<img src="https://wangfengyi0228.github.io/post-images/1636786857959.webp" alt="" loading="lazy"><br>
Dearest,<br>
there are no accidents,<br>
and he would've found us one way or another.<br>
Everything comes full circle.<br>
Be grateful it was sooner rather than later.<br>
<img src="https://wangfengyi0228.github.io/post-images/1636786862231.webp" alt="" loading="lazy"><br>
You'll think it harsh of me to say so,<br>
but no explanation I offer will satisfy you.<br>
Please don't be angry<br>
when I tell you that you seek resolutionsand explanations because you're young.<br>
But you will understand this one day.<br>
And when it happens,<br>
I want you to imagine me there to greet you,<br>
our lives stretched out ahead of us,<br>
a perpetual sunrise.<br>
<img src="https://wangfengyi0228.github.io/post-images/1636786868051.webp" alt="" loading="lazy"><br>
l have much to do,<br>
and you, my darling, even more.<br>
Please believe that I would do anything to see you happy.<br>
And so, I do the only thing I can.<br>
I release you.<br>
<img src="https://wangfengyi0228.github.io/post-images/1636786875509.webp" alt="" loading="lazy"><br>
My angel, flung out of space.<br>
<img src="https://wangfengyi0228.github.io/post-images/1636786881174.webp" alt="" loading="lazy"><br>
I took what you give willingly.<br>
<img src="https://wangfengyi0228.github.io/post-images/1636786887785.webp" alt="" loading="lazy"></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Movies | About Time]]></title>
        <id>https://wangfengyi0228.github.io/post/movies-or-about-time/</id>
        <link href="https://wangfengyi0228.github.io/post/movies-or-about-time/">
        </link>
        <updated>2021-09-06T06:47:42.000Z</updated>
        <content type="html"><![CDATA[<p>As so he told me his secret formula for happiness.<br>
Part one of the two part plans was that I should just get on with ordinary life, living it day by day, like anyone else.<br>
But then came part two of Dad' s plan.<br>
He told me to live every day again almost exatly the same.<br>
The first time with all the tensions and worries that stop us noticing how sweet the wold can be, but the second time noticing.<br>
<img src="https://wangfengyi0228.github.io/post-images/1636786282926.jpg" alt="" loading="lazy"><br>
How long will I love you?<br>
As long as stars are above you, and longer if I can.<br>
How long will I need you?<br>
As long as the seasons need to follow their plan.<br>
How long will I be with you?<br>
As long as the sea is bound to wash upon the sand.<br>
How long will I want you?<br>
As long as you want me to, and longer by far.<br>
How long will I hold you?<br>
As long as your father told you, as long as you can.<br>
How long will I give to you?<br>
As long as I live through you.<br>
<img src="https://wangfengyi0228.github.io/post-images/1636786288572.jpg" alt="" loading="lazy"><br>
I know it doesn't seem that way.<br>
But maybe it's the perfect day.<br>
Even though the bills are piling.<br>
Maybe Lady Luck ain't smiling.<br>
But if we'd only open our eyes, we'd see the blessings in disguise.<br>
That all the rain clouds are fountains.<br>
Though our troubles seem like mountains.<br>
There's gold in them hills.<br>
There's gold in them hills.<br>
So don't lose heart.<br>
Give the day a chance to start.<br>
<img src="https://wangfengyi0228.github.io/post-images/1636786294368.jpg" alt="" loading="lazy"><br>
I' d only give one piece of advice to anyone marring.<br>
We' re all quite similar in the end.<br>
We all get old and tell the same tales too many times.<br>
But try and marry someone kind.<br>
And there is a kind man with a good heart.<br>
I' m not particularly proud of many things in my life, but I am very proud to be the father of my son.<br>
<img src="https://wangfengyi0228.github.io/post-images/1636786301114.jpg" alt="" loading="lazy"><br>
Suddenly, time travel seems almost unnecessary, because every detail of life is so delightful.<br>
And in the end, I think I' ve learned the final lesson from my travels in time.<br>
The truth is, I now don' t travel back at all.<br>
Not even for the day.<br>
I just try to live every day, as if I' ve deliberately come back to this one day to enjoy it, as if it was the full final day of my extraordinary, ordinary life.<br>
<img src="https://wangfengyi0228.github.io/post-images/1636786305024.jpg" alt="" loading="lazy"><br>
Life' s a mixed bag, no matter who you are.<br>
We' re all travelling through time together, every day of our lives.<br>
All we can do is do our best, to relish this remarkable ride.</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Qt | 智能巡检机器人项目]]></title>
        <id>https://wangfengyi0228.github.io/post/qt-or-zhi-neng-xun-jian-ji-qi-ren-xiang-mu/</id>
        <link href="https://wangfengyi0228.github.io/post/qt-or-zhi-neng-xun-jian-ji-qi-ren-xiang-mu/">
        </link>
        <updated>2021-08-28T03:38:37.000Z</updated>
        <content type="html"><![CDATA[<blockquote>
<p>notes:该机器人使用卫星进行经纬度定位，适用于开阔地带，初始化搜星数达到19以上为佳，距离精度精确到小数点后16位（以米为单位）<br>
<strong>项目总体框图：</strong><br>
<img src="https://wangfengyi0228.github.io/post-images/1636688828688.png" alt="" loading="lazy"><br>
<strong>逻辑流程图：</strong><br>
<img src="https://wangfengyi0228.github.io/post-images/1636691559203.png" alt="" loading="lazy"><br>
<strong>ChasisSender</strong><br>
ChasisSender构造即启动两个串口（Imu串口和Cmd串口），Imu串口负责接收Imu数据和定位数据，将数据解析后发送到LtkjCenter。Cmd串口负责发送控制指令（速度和转角）到底盘控制器，自定义格式为“$cmd,转角,速度\r\n”。Cmd串口数据的发送在接收到LtkjCenter的control_msg信号后，开始发送，接收到control_msg.cmd_stop信号后关闭cmd串口，停止发送。</p>
</blockquote>
<ul>
<li>Signal:Chasis_state_signal(ChasisState) 向LtkjCenter发送底盘状态</li>
<li>Signal:Car_position_signal(Position) 向LtkjCenter发送车辆位置</li>
<li>Slot:ProcessImuMsg_slot() 接收Imu串口消息</li>
</ul>
<pre><code>void ChasisSender::ProcessImuMsg_slot()
{
    if(!imu_serial-&gt;canReadLine())   return ;
    QByteArray readData = imu_serial-&gt;readAll();
    if(readData.isEmpty())  return ;

    if(readData[0] == '$')
    {
        QString ImuData_str = QString(readData);
        QStringList tmpImuData_Lis = ImuData_str.split(&quot;,&quot;);

        if (tmpImuData_Lis.count() == 24)
        {
            QString imuYawangle_str = tmpImuData_Lis[3];
            QString imuLatitude_str = tmpImuData_Lis[12];
            QString imuLongitude_str = tmpImuData_Lis[13];

            Position tmp_position(0, 0, 0);
            tmp_position.x = imuLongitude_str.toDouble();
            tmp_position.y = imuLatitude_str.toDouble();
            tmp_position.theta = imuYawangle_str.toDouble();

            WGS2CART(car_position_,tmp_position);
            car_position_.x -= Origin_position_.x;
            car_position_.y -= Origin_position_.y;

            qDebug() &lt;&lt; &quot;x: &quot; &lt;&lt; car_position_.x &lt;&lt; &quot; y: &quot; &lt;&lt; car_position_.y &lt;&lt; &quot; z: &quot; &lt;&lt; car_position_.theta;

            chasis_state_.imu_state_ = 1;
            emit Chasis_state_signal(chasis_state_); 
            emit Car_position_signal(car_position_);  
       }
    }
    imu_serial-&gt;clear();
}

•Slot:ReceiveCmdMsg_slot(control_msg) 接收LtkjCenter控制指令
void ChasisSender::ReceiveCmdMsg_slot(control_msg msg)  
{
    if(!msg.cmd_stop)
    {
        if(cmd_serial-&gt;isOpen())
        {
            cmd_angle = msg.cmd_angle;
            cmd_speed = msg.cmd_speed;
            SendCmdSerialData();
        }
        else
        {
            OpenCmdSerialport();
        }
    }
    else
    {
        if(cmd_serial-&gt;isOpen())
        {
            CloseCmdSerialport();
        }
    }
}
</code></pre>
<p><strong>UdpReceiver</strong><br>
UdpReceiver构造即启动udp接收，具体绑定的IP和端口需要在构造函数中修改，这里不再提供修改的接口。接收的槽函数已经写好，解析需要根据协议来定，这个留着后期再加。测试阶段只保证实现功能。在udp接收槽接收到消息之后，进行解析，发送任务指令到LtkjCenter。</p>
<ul>
<li>Signal:CommandSingal(task_msg) 向LtkjCenter发送任务命令</li>
<li>Slot:Udp_receiver_slot() 接收udp消息并进行解析</li>
<li>Slot:Udp_position_slot(Position); 接收LtkjCenter车辆位置信息</li>
<li>Slot:Send_Timer_slot() 向LtkjCenter发送心跳</li>
</ul>
<pre><code>void UdpReceiver::Udp_receiver_slot() 
{
    while (udp_receiver_-&gt;hasPendingDatagrams()) 
    {
        QByteArray datagram;
        datagram.resize(udp_receiver_-&gt;pendingDatagramSize( ));
        udp_receiver_-&gt;readDatagram (datagram.data( ), datagram.size());

        if(datagram[0] == '$')
        {
            QString udp_str = QString(datagram);
            QStringList udp_str_list = udp_str.split(&quot;,&quot;);
            if(udp_str_list.count() == 4)
            {
                action = udp_str_list[1].toInt();
                end_pos_.setX(udp_str_list[2].toDouble());
                end_pos_.setY(udp_str_list[3].toDouble());
            }
        }
        // qDebug() &lt;&lt; datagram;
        task_msg_.state = action;
        task_msg_.point_target = end_pos_;
        emit CommandSingal(task_msg_);
    }
}
</code></pre>
<p><strong>LtkjCenter</strong><br>
LtkjCenter包含了LtkjMap和LtkjControl两个类。<br>
逻辑见流程图</p>
<ul>
<li>Signal:Chasis_cmd_signal(control_msg) 向ChasisSender发送控制指令，转角和速度以及停止信号</li>
<li>Slot:Command_slot(QString cmd) 接收UdpReceiver任务消息</li>
</ul>
<pre><code>void LtkjCenter::Command_slot(task_msg cmd)
{
    qDebug() &lt;&lt; &quot;rece from udp: &quot; &lt;&lt; cmd.state &lt;&lt; &quot; and &quot; &lt;&lt; cmd.point_target;
    if(cmd.state == 0)
    {
        if(track_timer-&gt;isActive())
        {
            track_timer-&gt;stop();
        }
        SetReady();
    }
    else if(cmd.state == 1)
    {
        end_position_.x = cmd.point_target.x();
        end_position_.y = cmd.point_target.y();
        end_position_.theta = 0;

        if(map_path_state_.map_state_ == 1 &amp;&amp; chasis_state_.imu_state_ == 1)
        {
            Position2QPointF(car_point, car_position_);
            Position2QPointF(end_point, end_position_);
            map_.AStar(car_point, end_point);           
            UpdateMapState();
        }
        else
        {
            qDebug() &lt;&lt; &quot;Error: map_state = 0 !&quot;;
            SetReady();
            return ;
        }

        if(map_path_state_.path_state_ == 1)
        {
            if(!track_timer-&gt;isActive())
            {
                track_timer-&gt;start(100);
            }
            else
            {
                SetReady();
                track_timer-&gt;start(100);
                qDebug() &lt;&lt; &quot;Warning: New Task Failed, Track task is in Progress&quot;;
            }

        }
        else
        {
            qDebug() &lt;&lt; &quot;Error: map_path = 0!&quot;;
            return ;
        }
    }
}
</code></pre>
<ul>
<li>Slot:Chasis_state_slot(ChasisState) 接收ChasisSender底盘状态消息<pre><code></code></pre>
</li>
</ul>
<p>void LtkjCenter::Chasis_state_slot(ChasisState state)<br>
{<br>
chasis_state_.imu_state_ = state.imu_state_;<br>
}```</p>
<ul>
<li>Slot:Chasis_position_slot(Position) 接收ChasisSender车辆位置消息</li>
</ul>
<pre><code class="language-void">{
    car_position_.x = pos.x;
    car_position_.y = pos.y;
    car_position_.theta = pos.theta;

    emit Position_signal(car_position_);
}```
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Practice | Missing number]]></title>
        <id>https://wangfengyi0228.github.io/post/practice-or-missing-number/</id>
        <link href="https://wangfengyi0228.github.io/post/practice-or-missing-number/">
        </link>
        <updated>2021-04-26T04:40:22.000Z</updated>
        <content type="html"><![CDATA[<p>Given a positive integer n(n≤40), pick n-1 numbers randomly from 1 to n and concatenate them in random order as a string s, which means there is a missing number between 1 and n. Can you find the missing number?(Notice that in some cases the answer will not be unique, and in these cases you only need to find one valid answer.）</p>
<h2 id="sample-input-1">Sample Input 1:</h2>
<p>20<br>
81971112205101569183132414117</p>
<h2 id="sample-output-1">Sample Output 1:</h2>
<p>16</p>
<h1 id="solution">Solution:</h1>
<pre><code>#include &lt;bits/stdc++.h&gt;

using namespace std;

int len,n;
bool f[100];
string s;
int num[100];

void dfs(int k)
{
    if(k&gt;=len)
    {
        for(int i=1;i&lt;=n;i++)
        {
            if(f[i]==false)
            {
                printf(&quot;%d\n&quot;,i);
            }
        }
        return;
    }
    if(f[num[k]]==false)
    {
        f[num[k]]=true;
        dfs(k+1);
        f[num[k]]=false;
    }
    if(f[num[k]*10+num[k+1]]==false&amp;&amp;num[k]*10+num[k+1]&lt;=n)
    {
        f[num[k]*10+num[k+1]]=true;
        dfs(k+2);
        f[num[k]*10+num[k+1]]=false;
    }
}

int main()
{
    scanf(&quot;%d&quot;,&amp;n);
    cin&gt;&gt;s;
    for(int i=0;i&lt;s.size();i++)
    {
        num[i]=s[i]-'0';
    }
    len=s.size();
    dfs(0);
    return 0;
}
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Practice | Count number of binary strings without consecutive 1’s]]></title>
        <id>https://wangfengyi0228.github.io/post/practice-or-count-number-of-binary-strings-without-consecutive-1s/</id>
        <link href="https://wangfengyi0228.github.io/post/practice-or-count-number-of-binary-strings-without-consecutive-1s/">
        </link>
        <updated>2021-03-16T04:43:16.000Z</updated>
        <content type="html"><![CDATA[<p>Given a positive integer n(3≤n≤90), count all possible distinct binary strings of length n such that there are no consecutive 1's .</p>
<h2 id="sample-input-1">Sample Input 1:</h2>
<p>2</p>
<h2 id="sample-output-1">Sample Output 1:</h2>
<p>3 // The 3 strings are 00, 01, 10</p>
<h2 id="sample-input-2">Sample Input 2:</h2>
<p>3</p>
<h2 id="sample-output-2">Sample Output 2:</h2>
<p>5 // The 5 strings are 000, 001, 010, 100, 101</p>
<h1 id="solution">Solution:</h1>
<pre><code>#include &lt;bits/stdc++.h&gt;

using namespace std;

int pao(int n)
{
    if(n==1)
    {
        return 2;
    }
    else if(n==2)
    {
        return 3;
    }
    else
    {
        return pao(n-1)+pao(n-2);
    }
}

int main()
{
    int n;
    scanf(&quot;%d&quot;,&amp;n);
    int dp[100][2];
    dp[1][0]=1;
    dp[1][1]=1;
    for(int i=2;i&lt;=n;i++)
    {
        dp[i][0]=dp[i-1][0]+dp[i-1][1];
        dp[i][1]=dp[i-1][0];
    }
    printf(&quot;%d\n&quot;,dp[n][0]+dp[n][1]);
    printf(&quot;%d\n&quot;,pao(n));
    return 0;
}
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[OD | YOLO]]></title>
        <id>https://wangfengyi0228.github.io/post/od-or-yolo/</id>
        <link href="https://wangfengyi0228.github.io/post/od-or-yolo/">
        </link>
        <updated>2020-12-28T04:47:44.000Z</updated>
        <content type="html"><![CDATA[<p>参考文献</p>
<ul>
<li>[1]Redmon J , Divvala S , Girshick R , et al. You Only Look Once: Unified, Real-Time Object Detection[C]// Computer Vision &amp; Pattern Recognition. IEEE, 2016.</li>
<li>[2]Redmon J , Farhadi A . YOLO9000: Better, Faster, Stronger[C]// IEEE Conference on Computer Vision &amp; Pattern Recognition. IEEE, 2017:6517-6525.</li>
<li>[3]Redmon J , Farhadi A . YOLOv3: An Incremental Improvement[J]. arXiv e-prints, 2018.</li>
<li>[4]Bochkovskiy A , Wang C Y , Liao H Y M . YOLOv4: Optimal Speed and Accuracy of Object Detection[J]. 2020.</li>
</ul>
<h1 id="1yolov1">1.YOLOv1</h1>
<p><strong>1.1 概述</strong><br>
人类的视觉系统快而准确，只要看一眼图像就能知道图像中有什么物体，以及它们之间的相互关系。YOLO（You Only Look Once: Unified, Real-Time Object Detection）正是为了能够进行这样快速、准确的目标检测而诞生的，它创造性地将边界框和类别两个过程合二为一。与基于滑动窗口和region proposal的R-CNN、Fast R-CNN等算法不同，<code>YOLO训练和检测均是在一个单独网络中进行，没有求取region proposal的过程，而是将目标检测作为回归问题求解，基于一个单独的end-to-end网络，完成从原始图像的输入到物体位置和类别的输出。</code><br>
YOLO首先将图片分成<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>S</mi><mo>∗</mo><mi>S</mi></mrow><annotation encoding="application/x-tex">S*S</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.05764em;">S</span></span></span></span>个大小相同的grid，然后让每个grid预测出B个bounding boxes，每个边界框预测一个物体的中心位置(x,y)、高(h)、宽(w)，以及这次预测的置信度，每个grid识别出一种物体。若分类器可以识别C种不同的物体，那么预测结果总共有<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>S</mi><mo>∗</mo><mi>S</mi><mo>∗</mo><mo>(</mo><mi>B</mi><mo>∗</mo><mn>5</mn><mo>+</mo><mi>C</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">S*S*(B*5+C)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">5</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="mclose">)</span></span></span></span>个张量。<br>
<img src="https://wangfengyi0228.github.io/post-images/1636695019326.png" alt="" loading="lazy"><br>
上图展示了图像的分割过程，以及每一个grid预测出的bounding boxes的情况。YOLO的网络结构包括24个卷积层和2个全连接层，其中，卷积层用来提取图像特征，全连接层用来预测图像位置和类别概率值。YOLO网络借鉴了GoogLeNet分类网络结构。不同的是，YOLO未使用inception module，而是使用1x1卷积层（此处1x1卷积层的存在是为了跨通道信息整合）+3x3卷积层简单替代。<br>
<img src="https://wangfengyi0228.github.io/post-images/1636695041225.png" alt="" loading="lazy"><br>
<img src="https://wangfengyi0228.github.io/post-images/1636695047630.png" alt="" loading="lazy"><br>
第一项是物体中心坐标的损失函数，使用的是简单的均方误差；第二项是高和宽的损失函数，使用了平方根的均方误差，是为了使模型对较大的和较小的物体同样敏感，反映出大box中的小偏差比小box中的小偏差更重要。第三项是边框内有对象时的置信度误差，第四项是边框内无对象时的置信度误差，第五项是对象分类误差。<br>
在计算IOU误差时，包含物体的grid与不包含物体的grid，二者的IOU误差对网络loss的贡献值是不同的。两者若采用相同的权值，那么不包含物体的格子的confidence值近似为0，变相放大了包含物体的grid的confidence误差在计算网络参数梯度时的影响，可能会导致模型不稳定。为解决这一问题，YOLO的做法是<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>s</mi><mi>e</mi><mi>t</mi><msub><mi>λ</mi><mrow><mi>c</mi><mi>o</mi><mi>o</mi><mi>r</mi><mi>d</mi></mrow></msub><mo>=</mo><mn>5</mn><mi>a</mi><mi>n</mi><mi>d</mi><msub><mi>λ</mi><mrow><mi>n</mi><mi>o</mi><mi>o</mi><mi>b</mi><mi>j</mi></mrow></msub><mo>=</mo><mn>5</mn></mrow><annotation encoding="application/x-tex">set \lambda _{coord}=5 and \lambda _{noobj}=5</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord mathdefault">s</span><span class="mord mathdefault">e</span><span class="mord mathdefault">t</span><span class="mord"><span class="mord mathdefault">λ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">c</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight" style="margin-right:0.02778em;">r</span><span class="mord mathdefault mtight">d</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.980548em;vertical-align:-0.286108em;"></span><span class="mord">5</span><span class="mord mathdefault">a</span><span class="mord mathdefault">n</span><span class="mord mathdefault">d</span><span class="mord"><span class="mord mathdefault">λ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">n</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">b</span><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">5</span></span></span></span>来进行修正。<br>
<strong>1.2 YOLOv1的优点</strong><br>
YOLO将识别与定位合二为一，结构简洁，检测速度快。相对于R-CNN系列, YOLO的整个流程中都能看到整张图像的信息，因此它在检测物体时能很好的利用上下文信息，从而不容易在背景上预测出错误的物体信息。同时YOLO可以学习到高度泛化的特征，能将一个域上学到的特征迁移到不同但相关的域上，如在自然图像上做训练的YOLO，在艺术图片上可以得到较好的测试结果。</p>
<p><strong>1.3 YOLOv1的缺点</strong><br>
YOLOv1中一个grid只能识别出一种物体，且bounding box的预测准确度不如R-CNN等Region-based方法，其对小物体的检测，尤其是对密集的小物体检测效果较差。</p>
<h1 id="2yolov2">2.YOLOv2</h1>
<p>作者首先在YOLOv1的基础上提出了改进的YOLOv2，然后提出了一种检测与分类联合训练的方法，使用这种联合训练方法在COCO检测数据集和ImageNet分类数据集上训练出了YOLO9000模型，其可以检测超过9000多类物体。<br>
<strong>2.1 YOLOv2的改进策略</strong><br>
YOLOv1虽然检测速度很快，但是在检测精度上却不如R-CNN系检测方法，YOLOv1在物体定位方面不够准确，并且召回率较低。YOLOv2共提出了几种改进策略来提升YOLO模型的定位准确度和召回率，从而提高mAP，YOLOv2在改进中遵循一个原则：保持检测速度，这也是YOLO模型的一大优势。</p>
<ul>
<li>
<p>(1)Batch Normalization<br>
Batch Normalization有助于规范化模型，可以提升模型的收敛速度，而且可以起到一定的正则化效果，降低模型的过拟合。<code>在YOLOv2中，每个卷积层后面都添加了Batch Normalization层，并且不再使用droput。</code>使用Batch Normalization后，YOLOv2的mAP提升了2.4%。</p>
</li>
<li>
<p>(2)High Resolution Classifier<br>
目前大部分的检测模型都会在先在ImageNet分类数据集上预训练模型的主体部分，大多数分类器操作的输入图像小于256×256，分辨率相对较低，不利于检测模型。所以YOLOv1在采用224×224分类模型预训练后，将分辨率增加至448×448。但是直接切换分辨率，检测模型可能难以快速适应高分辨率。<code>所以YOLOv2增加了在ImageNet数据集上使用448×448的分辨率对分类网络进行10个epoch的精细调整，这给了网络时间来调整它的过滤器，以更好地工作在更高分辨率的输出，</code>然后在检测时对对得到的网络进行微调，可以使得模型适用高分辨率输入。使用高分辨率分类器后，YOLOv2的mAP提升了约4%。</p>
</li>
<li>
<p>(3)Convolutional With Anchor Boxes<br>
YOLOv1识别精度低的其中一个原因就是边界框的宽与高的确定存在问题。YOLOv1最后采用的是全连接层直接对边界框进行预测，其中边界框的宽与高是相对整张图片大小的，而由于各个图片中存在不同尺度和长宽比的物体，因此YOLOv1在训练过程中学习适应不同物体的形状是比较困难的，这也导致YOLOv1在精确定位方面表现较差。YOLOv2借鉴了Faster R-CNN中RPN网络的先验框策略。<code>RPN对CNN特征提取器得到的特征图进行卷积来预测每个位置的边界框以及置信度，并且各个位置设置不同尺度和比例的先验框，采用先验框使得模型更容易学习。</code><br>
所以YOLOv2移除了YOLOv1中的全连接层而采用了卷积和anchor boxes来预测边界框。为了使检测所用的特征图分辨率更高，移除其中的一个pool层。在检测模型中，YOLOv2不是采448×448图片作为输入，而是采用416×416大小。因为YOLOv2模型下采样的步长为32，对于416×416大小的图片，最终得到的特征图大小为13×13，维度是奇数，这样特征图恰好只有一个中心位置。对于一些大物体，它们中心点往往落入图片中心位置，此时使用特征图的一个中心点去预测这些物体的边界框相对容易些。<br>
使用anchor boxes之后，YOLOv2的mAP有稍微下降，但召回率有所上升。</p>
</li>
<li>
<p>(4)Dimension Clusters<br>
在Faster R-CNN和SSD中，先验框的长和宽是手动设定的，带有一定的主观性。<code>如果选取的先验框维度比较合适，那么模型更容易学习，从而做出更好的预测。因此，YOLOv2采用k-means聚类方法对训练集中的边界框做了聚类分析。</code>因为设置先验框的主要目的是为了使得预测框与ground truth的IOU更好，所以聚类分析时选用box与聚类中心box之间的IOU值作为距离指标：<br>
<img src="https://wangfengyi0228.github.io/post-images/1636695702119.png" alt="" loading="lazy"></p>
</li>
<li>
<p>(5) New Network：Darknet-19<br>
YOLOv2采用了一个新的基础模型（特征提取器），称为Darknet-19，包括19个卷积层和5个maxpooling层，如Table 6所示。Darknet-19与VGG16模型设计原则是一致的，主要采用3<em>3卷积，采用2</em>2的maxpooling层之后，特征图维度降低2倍，而同时将特征图的channles增加两倍。与NIN(Network in Network)类似，Darknet-19最终采用global avgpooling做预测，并且在3<em>3卷积之间使用1</em>1卷积来压缩特征图channles以降低模型计算量和参数。Darknet-19每个卷积层后面同样使用了batch norm层以加快收敛速度，降低模型过拟合。在ImageNet分类数据集上，Darknet-19的top-1准确度为72.9%，top-5准确度为91.2%，但是模型参数相对小一些。使用Darknet-19之后，YOLOv2的mAP值没有显著提升，但是计算量却可以减少约33%。<br>
<img src="https://wangfengyi0228.github.io/post-images/1636695596885.png" alt="" loading="lazy"></p>
</li>
<li>
<p>(6)Fine-Grained Features<br>
YOLOv2的输入图片大小为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>416</mn><mo>∗</mo><mn>416</mn></mrow><annotation encoding="application/x-tex">416*416</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">4</span><span class="mord">1</span><span class="mord">6</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">4</span><span class="mord">1</span><span class="mord">6</span></span></span></span>，经过5次maxpooling之后得到<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>13</mn><mo>∗</mo><mn>13</mn></mrow><annotation encoding="application/x-tex">13*13</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord">3</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord">3</span></span></span></span>大小的特征图，并以此特征图采用卷积做预测。<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>13</mn><mo>∗</mo><mn>13</mn></mrow><annotation encoding="application/x-tex">13*13</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord">3</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord">3</span></span></span></span>大小的特征图对检测大物体是足够了，但是对于小物体还需要更精细的特征图（Fine-Grained Features）。因此SSD使用了多尺度的特征图来分别检测不同大小的物体，前面更精细的特征图可以用来预测小物体。YOLOv2提出了一种passthrough层来利用更精细的特征图。YOLOv2所利用的Fine-Grained Features是<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>26</mn><mo>∗</mo><mn>26</mn></mrow><annotation encoding="application/x-tex">26*26</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">2</span><span class="mord">6</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">2</span><span class="mord">6</span></span></span></span>大小的特征图（最后一个maxpooling层的输入），对于Darknet-19模型来说就是大小为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>26</mn><mo>∗</mo><mn>26</mn><mo>∗</mo><mn>512</mn></mrow><annotation encoding="application/x-tex">26*26*512</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">2</span><span class="mord">6</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">2</span><span class="mord">6</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">5</span><span class="mord">1</span><span class="mord">2</span></span></span></span>的特征图。passthrough层与ResNet网络的shortcut类似，以前面更高分辨率的特征图为输入，然后将其连接到后面的低分辨率特征图上。前面的特征图维度是后面的特征图的2倍，passthrough层抽取前面层的每个<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>2</mn><mo>∗</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">2*2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">2</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">2</span></span></span></span>的局部区域，然后将其转化为channel维度，对于<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>26</mn><mo>∗</mo><mn>26</mn><mo>∗</mo><mn>512</mn></mrow><annotation encoding="application/x-tex">26*26*512</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">2</span><span class="mord">6</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">2</span><span class="mord">6</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">5</span><span class="mord">1</span><span class="mord">2</span></span></span></span>的特征图，经passthrough层处理之后就变成了<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>13</mn><mo>∗</mo><mn>13</mn><mo>∗</mo><mn>2048</mn></mrow><annotation encoding="application/x-tex">13*13*2048</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord">3</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord">3</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">2</span><span class="mord">0</span><span class="mord">4</span><span class="mord">8</span></span></span></span>的新特征图（特征图大小降低4倍，而channles增加4倍，图6为一个实例），这样就可以与后面的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>13</mn><mo>∗</mo><mn>13</mn><mo>∗</mo><mn>1024</mn></mrow><annotation encoding="application/x-tex">13*13*1024</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord">3</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord">3</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord">0</span><span class="mord">2</span><span class="mord">4</span></span></span></span>特征图连接在一起形成<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>13</mn><mo>∗</mo><mn>13</mn><mo>∗</mo><mn>3072</mn></mrow><annotation encoding="application/x-tex">13*13*3072</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord">3</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord">3</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">3</span><span class="mord">0</span><span class="mord">7</span><span class="mord">2</span></span></span></span>的特征图，然后在此特征图基础上卷积做预测。在YOLO的C源码中，passthrough层称为reorg layer。在TensorFlow中，可以使用tf.extract_image_patches或者tf.space_to_depth来实现passthrough层。<br>
#3.YOLOv3<br>
YOLOv3借鉴了一些好的方案融合到YOLO里面，在保持速度优势的前提下，提升了预测精度，尤其是加强了对小物体的识别能力。</p>
</li>
<li>
<p><strong>DBL：</strong> 代码中的Darknetconv2d_BN_Leaky，是yolo_v3的基本组件,就是卷积+BN+Leaky relu。对于v3来说，BN和leaky relu已经是和卷积层不可分离的部分了(最后一层卷积除外)，共同构成了最小组件。</p>
</li>
<li>
<p>**resn： **n代表数字，有res1，res2, … ,res8等等，表示这个res_block里含有多少个res_unit。这是yolo_v3的大组件，yolo_v3开始借鉴了ResNet的残差结构，使用这种结构可以让网络结构更深(从v2的darknet-19上升到v3的darknet-53，前者没有残差结构)。res_block的基本组件也是DBL。</p>
</li>
<li>
<p>**concat： *<em>张量拼接。将darknet中间层和后面的某一层的上采样进行拼接。拼接的操作和残差层add的操作是不一样的，拼接会扩充张量的维度，而add只是直接相加不会导致张量维度的改变。<br>
<strong>(1)backbone：darknet-53</strong><br>
为了达到更好的分类效果，作者自己设计训练了darknet-53。作者在ImageNet上实验发现这个darknet-53，的确很强，相对于ResNet-152和ResNet-101，darknet-53不仅在分类精度上差不多，计算速度还比ResNet-152和ResNet-101快得多，网络层数也较少。<code>YOLOv3使用了darknet-53的前面的52层（没有全连接层），yolo_v3这个网络是一个全卷积网络，大量使用残差的跳层连接，并且为了降低池化带来的梯度负面效果，作者直接摒弃了Pooling，用conv的stride来实现降采样。</code>在这个网络结构中，使用的是步长为2的卷积来进行降采样。<br>
为了加强算法对小目标检测的精确度，YOLO v3中采用类似FPN的upsample和融合做法（最后融合了3个scale，其他两个scale的大小分别是26×26和52×52），在多个scale的feature map上做检测。<br>
<strong>(2)Output</strong><br>
yolo v3输出了3个不同尺度的feature map，即predictions across scales，这个借鉴了FPN(feature pyramid networks)<code>采用多尺度来对不同size的目标进行检测，越精细的grid cell就可以检测出越精细的物体。</code>3个尺度的feature mapy1,y2和y3的深度都是255，边长的规律是13:26:52。对于COCO类别而言，有80个种类，所以每个box应该对每个种类都输出一个概率。yolo v3设定的是每个网格单元预测3个box，所以每个box需要有(x, y, w, h, confidence)五个基本参数，然后还要有80个类别的概率，3</em>(5 + 80) = 255。而yolo v1的输出张量是7x7x30，只能识别20类物体，显然YOLOv3改进了许多。<br>
<strong>(3)Bounding Box</strong><br>
YOLOv3的Bounding Box由YOLOv2又做出了更好的改进。在YOLOv2和YOLOv3中，都对图像中的object采用了k-means聚类。 feature map中的每一个cell都会预测3个边界框（bounding box），每个bounding box都会预测三个东西：每个框的位置（4个值，中心坐标tx和ty，框的高度bh和宽度bw）、一个objectness prediction 、N个类别，coco数据集80类，voc20类。三次检测，每次对应的感受野不同，适合检测的物体大小不同。<br>
<img src="https://wangfengyi0228.github.io/post-images/1636695931976.png" alt="" loading="lazy"><br>
<strong>(4)LOSS Function</strong><br>
YOLOv3重要改变之一：No more softmaxing the classes。YOLO v3对图像中检测到的对象执行多标签分类。logistic回归用于对anchor包围的部分进行一个目标性评分(objectness score)，即这块位置是目标的可能性有多大。这一步是在predict之前进行的，可以去掉不必要的anchor，减少计算量。不同于faster R-CNN的是，yolo_v3只会对1个prior进行操作，也就是那个最佳prior。而logistic回归就是用来从9个anchor priors中找到objectness score(目标存在可能性得分)最高的那一个。logistic回归就是用曲线对prior相对于 objectness score映射关系的线性建模。</p>
</li>
</ul>
<h1 id="4yolov4">4.YOLOv4</h1>
<p><strong>(1)Bag of freebies</strong><br>
在目标检测中Bag of freebies是指：用一些比较有用的训练技巧来训练模型，从而使目标检测器在不增加推理损耗的情况下达到更好的精度。目标检测经常采用这种方法，并符合这个定义的就是数据增强。<code>数据增强的目的是：增加输入图像的可变性，从而使设计的目标检测模型对不同环境的图片具有较高的鲁棒性。</code><br>
其他的一些Bag of freebies方法是专门解决数据集中的语义分布偏差。在处理语义困扰的问题上，有一个很重要的问题是不同类别之间的数据不平衡，而two-stage 检测器处理这个问题通常是通过hard negative example mining或online hard example mining 。 最后一个bag of freebies是objective function of Bounding Box (BBox) 回归。检测器通常使用MSE对BBOX的中心点和宽高进行回归，至于anchor-based方法，它是为了估算出对应的偏移量。但是，要直接估计BBOX的点坐标值，是要将这些点作为独立变量，但实际上未考虑对象本身的完整性。为了使这一问题得到更好的处理，一些研究人员最近提出的IoU损失，同时考虑预测的BBox面积和ground truth BBox面积覆盖。</p>
<p><strong>(2)Bag of specials</strong><br>
Bag of specials是指一些plugin modules（例如特征增强模型，或者一些后处理），这部分增加的计算量很少，但是能有效地增加物体检测的准确率，将这部分称之为Bag of specials。这部分插件模块能够增强网络模型的一些属性，例如增大感受野（ASFF，ASPP，RFB这些模块）、引入注意力机制（spatial attention和channel attention）、增加特征集成能力。后处理算法是指用一些算法来筛选模型预测出来的结果，后处理方法常用的是NMS，它可以用于过滤那些预测错误的BBoxes，并只保留较高的候选BBoxes。</p>
<p><strong>(3)额外改进</strong><br>
使用一种新的数据增强方法Mosaic和Self-Adversarial Training (SAT)，前者把四张图拼成一张图来训练，变相的等价于增大了mini-batch；后者是在一张图上，让神经网络反向更新图像，对图像做改变扰动，然后在这个图像上训练。<br>
<img src="https://wangfengyi0228.github.io/post-images/1636696843641.png" alt="" loading="lazy"><br>
跨最小批的归一化（Cross mini-batch Normal），在CBN的基础上修改的SAM，从spatial-wise attention修改为point-wise attention：<br>
<img src="https://wangfengyi0228.github.io/post-images/1636696855738.png" alt="" loading="lazy"><br>
修改的PAN，把通道从相加（add）改变为concat：<br>
<img src="https://wangfengyi0228.github.io/post-images/1636696863193.png" alt="" loading="lazy"><br>
<strong>(4)YOLOv4的组成</strong><br>
Backbone: CSPDarknet53<br>
Neck: SPP , PAN<br>
Head: YOLOv3</p>
<h1 id="5总结">5.总结</h1>
<p><strong>(1)YOLOv1</strong></p>
<ul>
<li>训练和检测均是在一个单独网络中进行，没有求取region proposal的过程。</li>
<li>将目标检测作为回归问题求解，基于一个单独的end-to-end网络。</li>
<li>损失函数中的几个项与输出向量中的内容相对应。</li>
<li>YOLOv1检测速度快，但在检测小物体以及密集物体方面存在欠缺。<br>
<strong>(2)YOLOv2</strong></li>
<li>YOLOv2在保证检测速度的基础上提出了几种改进策略来提升YOLO模型的定位准确度和召回率。</li>
<li>网络结构中每个卷积层后面都添加了Batch Normalization层，并且使用高了分辨率分类器。</li>
<li>YOLOv2采用k-means聚类方法对训练集中的边界框做了聚类分析，使得选取的先验框维度更合适，模型更容易学习，从而做出更好的预测。<br>
<strong>(3)YOLOv3</strong></li>
<li>YOLOv3借鉴了一些好的方案融合到YOLO里面，在保持速度优势的前提下，提升了预测精度，尤其是加强了对小物体的识别能力。</li>
<li>Darknetconv2d_BN_Leaky是yolo_v3的基本组件,就是卷积+BN+Leaky relu。</li>
<li>YOLOv3开始借鉴了ResNet的残差结构，使用这种结构可以让网络结构更深。</li>
<li>yolo v3输出了3个不同尺度的feature map，采用多尺度来对不同size的目标进行检测，越精细的grid cell就可以检测出越精细的物体。</li>
<li>YOLOv3的Bounding Box由YOLOv2又做出了更好的改进。在YOLOv2和YOLOv3中，都对图像中的object采用了k-means聚类。 feature map中的每一个cell都会预测3个边界框。三次检测，每次对应的感受野不同，适合检测的物体大小不同。<br>
<strong>(4)YOLOv4</strong></li>
<li>YOLOv4研究设计了一个简单且高效的目标检测算法，该算法降低了训练门槛，使得普通人员在拥有一块1080TI或者2080TI的情况下就能够训练一个super fast and accurate 的目标检测器。</li>
<li>在训练过程中，验证了最新的Bag-of-Freebies和Bag-of-Specials对Yolo-V4的影响。</li>
<li>简化以及优化了一些最新提出的算法，包括（CBN，PAN，SAM），从而使Yolo-V4能够在一块GPU上就可以训练起来。</li>
</ul>
<p>YOLOv5初步移植到手机后的效果图：<br>
<img src="https://wangfengyi0228.github.io/post-images/1636696976292.png" alt="" loading="lazy"></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Movies | Call Me By Your Name]]></title>
        <id>https://wangfengyi0228.github.io/post/movies-or-call-me-by-your-name/</id>
        <link href="https://wangfengyi0228.github.io/post/movies-or-call-me-by-your-name/">
        </link>
        <updated>2020-12-06T06:56:46.000Z</updated>
        <content type="html"><![CDATA[<p>&quot;Is better to speak or to die?&quot;<br>
There's no one else I can say this to but you</p>
<p>Summer 1983<br>
<img src="https://wangfengyi0228.github.io/post-images/1636786745776.jpg" alt="" loading="lazy"><br>
My mom has been reading this 16th-certury French romance. She read some of it to my dad and I the night the lights went out.<br>
About the knight that doesn't know whether to speak or die?<br>
Right.<br>
So, does he doesn't he?<br>
&quot;Better to speak&quot; she said.<br>
<img src="https://wangfengyi0228.github.io/post-images/1636786741275.jpg" alt="" loading="lazy"><br>
You two had a nice friendship.<br>
Yeah.<br>
You're too smart not to know how rare, how special what you two had was.<br>
Oliver was Oliver.<br>
Because it was him. Because it was me.<br>
Oliver may be very intelligent but…<br>
Oh, no, no. He was more than intelligent. What you two had… had everything and nothing to do with intelligence. He was good. You were both lucky to have found each other, because… you, too, are good.<br>
<img src="https://wangfengyi0228.github.io/post-images/1636786737383.jpg" alt="" loading="lazy"><br>
Which flatters you both. You know, when you least expect it, nature has cunning ways of finding our weakest spot. Just remember, I'm here. Right now, you may not want to feel anything. And maybe it's not me you want to speak about these things, but uh… feel something, you obviously did. Look, you had a beautiful friendship. Maybe more than a friendship. And I envy you.<br>
<img src="https://wangfengyi0228.github.io/post-images/1636786731524.jpg" alt="" loading="lazy"><br>
In my place, most parents would hope the whole thing goes away, and pray that their sons land on their feet. But I am not such a parent. We rip out so much of ourselves to be cured of things faster that we go bankrupt by the age of thirty and have less to offer each time we start with someone new. But to feel nothing, so as not to feel anything. What a waste!<br>
<img src="https://wangfengyi0228.github.io/post-images/1636786727473.jpg" alt="" loading="lazy"><br>
How you live your life is your business. Just remember… Our hearts and our bodies are given to us only once. Most of us can't help but live as though we've got two lives to live, one is the mockup, the other the finished version, and then there are all those versions in between. But there's only one, and before you know it, your heart is worn out, and, as for your body, there comes a point when no one looks at it, much less wants to come near it. Right now, there's sorrow, pain. Don't kill it… and with it, the joy you felt.<br>
<img src="https://wangfengyi0228.github.io/post-images/1636786722800.jpg" alt="" loading="lazy"><br>
When you least expect it,nature has cunning ways of finding our weakest spot.<br>
<img src="https://wangfengyi0228.github.io/post-images/1636786719214.jpg" alt="" loading="lazy"><br>
Twenty years was yesterday, and yesterday was just earlier this morning, and morning seemed light-years away.<br>
<img src="https://wangfengyi0228.github.io/post-images/1636786713340.jpg" alt="" loading="lazy"></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[OD | 术语名词]]></title>
        <id>https://wangfengyi0228.github.io/post/od-or-zhu-yu-ming-ci/</id>
        <link href="https://wangfengyi0228.github.io/post/od-or-zhu-yu-ming-ci/">
        </link>
        <updated>2020-11-30T04:45:25.000Z</updated>
        <content type="html"><![CDATA[<p><strong>1.学习率</strong><br>
学习率是在梯度下降的过程中更新权重时的超参数，当学习率过大的时候会导致模型难以收敛，过小的时候会收敛速度过慢，学习率是一个十分重要的超参数，合理的学习率才能让模型收敛到最小点而非局部最优点或鞍点。</p>
<p><strong>2.损失函数</strong><br>
损失函数是将随机事件或其有关随机变量的取值映射为非负实数，以表示该随机事件的“风险”或“损失”的函数。</p>
<p><strong>3.召回率</strong><br>
召回率是针对原来的样本而言的，它表示的是样本中的正例有多少被预测正确了。预测有两种可能，一种是把原来的正类预测成正类(TP)，另一种就是把原来的正类预测为负类(FN)。则</p>
<p><strong>4.交并比（IOU）</strong><br>
交并比时在目标检测任务中，计算两个边界框（定位的边界框和实际边界框）的交集和并集之比。一般约定,0.5 是阈值，用来判断预测的边界框是否正确，IOU越高，边界框越精确。</p>
<p><strong>5.非极大值抑制</strong><br>
算法可能对同一个对象做出多次检测，非极大值抑制这个方法可以确保算法对每个对象只检测一次。首先看检测概率最大的边界框，进行标记，之后非极大值抑制就会逐一审视剩下的矩形，所有和这个最大的边框有很高交并比、高度重叠的其他边界框，输出就会被抑制。</p>
<p><strong>6.mAP(mean Average Precision)</strong><br>
查准率（Precision）: TP/(TP + FP);查全率（即各类别 AP 的平均值Recall）: TP/(TP + FN)。二者绘制的曲线称为 P-R 曲线。AP是PR 曲线下面积。mAP即各类AP的平均值，用来衡量目标检测中的识别精度。</p>
<p><strong>7.感受野（Receptive Field）</strong><br>
在卷积神经网络中，感受野（Receptive Field）的定义是卷积神经网络每一层输出的特征图（feature map）上的像素点在输入图片上映射的区域大小，即特征图上的一个点对应输入图上的区域。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[OD | R-CNN、SPP-net]]></title>
        <id>https://wangfengyi0228.github.io/post/od-or-r-cnnspp-ne/</id>
        <link href="https://wangfengyi0228.github.io/post/od-or-r-cnnspp-ne/">
        </link>
        <updated>2020-10-28T06:11:03.000Z</updated>
        <content type="html"><![CDATA[<h1 id="参考文献">参考文献</h1>
<ul>
<li>[1]Uijlings J R R, Van De Sande K E A, Gevers T, et al. Selective search for object recognition[J]. International journal of computer vision, 2013, 104(2): 154-171.</li>
<li>[2]Girshick R , Donahue J , Darrell T , et al. Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation[C]// CVPR. IEEE, 2014.</li>
<li>[3]He K, Zhang X, Ren S, et al. Spatial pyramid pooling in deep convolutional networks for visual recognition[J]. IEEE transactions on pattern analysis and machine intelligence, 2015, 37(9): 1904-1916<br>
#1.R-CNN<br>
R-CNN采取的总体思路是：首先输入一张图片，生成1K~2K个候选区域，对每个候选区域，使用CNN提取特征，接着采用svm算法对各个候选框中的物体进行分类识别，最后使用回归器精细修正候选框位置。<br>
<img src="https://wangfengyi0228.github.io/post-images/1636699933320.png" alt="" loading="lazy"><br>
<strong>1.1候选框的搜索</strong><br>
当输入一张图片时，要搜索出所有可能是物体的区域，这个采用的方法是传统文献的算法：selective search for object recognition。<s>这个算法的过程类似于构建赫夫曼树</s>。<br>
<strong>1.1.1selective search for object recognition</strong><br>
以往的目标检测的做法主要是基于穷举搜索（Exhaustive Search）：选择一个窗口扫描整张图像，改变窗口的大小，继续扫描整张图像。这种做法是比较原始直观，但改变窗口大小，扫描整张图像，非常耗时。若能过滤掉一些无用的box将会节省大量时间。这就是Selective Search(选择性搜索)的优点。<br>
选择性搜索(Selective Search)综合了穷举搜索（Exhausticve Search)和分割(Segmentation)的方法，意在找到一些可能的目标位置集合。作者将穷举搜索和分割结合起来，采取组合策略保证搜索的多样性，能够大幅度降低搜索空间，提高程序效率，减小计算量。<br>
在图像中，同一个物体在像素点尺度上具有一定的相似性，如颜色值相似性、纹理相似性、尺度相似性等等。Selective Search正是利用了同一物体在像素点尺度范围的相似性，不断的去合并一些达到预设相似性阈值的相邻像素点，从而将可能属于同一物体的像素点合并，形成一个区域box。这样将一张图像中所有具有一定相似性的像素点合并，形成一些可能属于同一物体的区域集，作为下一步用来检测的区域集，即可能的目标boxes。</li>
</ul>
<p>算法过程：<br>
(1)输入：彩色图片（三通道）。<br>
(2)获取初始分割区域R={r1,r2,…,rn}，初始化相似度集合S=∅。<br>
(3)计算R中两两相邻区域（ri,rj）之间的相似度，将其添加到相似度集合S中。<br>
(4)从相似度集合S中找出，相似度最大的两个区域ri和rj，将其合并成为一个区域 rt。然后从相似度集合中除去原先与ri和rj相邻区域之间计算的相似度。计算新的rt与其相邻区域（原先与ri或rj相邻的区域）的相似度，将其结果添加的到相似度集合S中。同时将新区域 rt 添加到区域集合R中。迭代直至S为空，即可合并区域的都已合并完。 区域的合并方式类似于哈夫曼树的构造过程，因此称之有层次（hierarchical）。<br>
(5)获取R中每个区域的Bounding Boxes，这个结果就是图像中物体可能位置的可能结果集合L。</p>
<p><strong>1.1.2缩放处理</strong><br>
候选框是矩形的，而且大小各不相同。然而CNN对输入图片的大小是固定的，因此对于每个输入的候选框都需要缩放到固定的大小。缩放共有两种方法。<br>
(1)各向异性缩放<br>
无论图片原来的长宽比例，无论它是否扭曲，直接进行缩放，全部缩放到CNN输入的大小277*277，如下图第四列。<br>
(2)各向同性缩放<br>
因为图片扭曲后，会对后续CNN的训练精度有影响，于是作者也测试了“各向同性缩放”方案，有两种办法：<br>
A.直接在原始图片中，把bounding box的边界进行扩展延伸成正方形，然后再进行裁剪；如果已经延伸到了原始图片的外边界，那么就用bounding box中的颜色均值填充(padding)，如下图第二列。<br>
B.先把bounding box图片裁剪出来，然后用固定的背景颜色填充成正方形图片(背景颜色也是采用bounding box的像素颜色均值)，如下图第三列。<br>
经过作者一系列实验表明采用padding=16的各向异性变形即下图第二行第三列效果最好，能使mAP提升3-5%。<br>
<img src="https://wangfengyi0228.github.io/post-images/1636699973235.png" alt="" loading="lazy"></p>
<p><strong>1.2 CNN特征提取</strong><br>
采用Alexnet网络及其初始参数，学习率=0.01进行预训练。接着采用selective search 搜索出来的候选框，处理到指定大小图片，继续对预训练的CNN模型进行fine-tuning训练。假设要检测的物体类别有N类，需要把预训练阶段的CNN模型的最后一层给替换掉，替换成N+1个输出的神经元(加1表示还有一个背景)，然后这一层直接采用参数随机初始化的方法，其它网络层的参数不变；接着继续做随机梯度下降法SGD训练。开始的时候，SGD学习率选择0.001，在每次训练的时候，batch size大小选择128，其中32个正样本、96个负样本。</p>
<p><strong>1.3 SVM训练</strong><br>
为了更好地定义正样本与负样本，作者测试了IOU阈值的各种方案数值0,0.1,0.2,0.3,0.4,0.5。最后通过训练发现，如果选择IOU阈值为0.3效果最好,即当IOU小于0.3的时候，就把它标注为负样本（选择为0时，精度下降了4个百分点，选择0.5时，精度下降了5个百分点）。一旦CNN f7层特征被提取出来，那么将为每个物体类训练一个svm分类器。当用CNN提取2000个候选框时，可以得到2000<em>4096这样的特征向量矩阵，然后只需要把这样的一个矩阵与svm权值矩阵4096</em>N点乘(N为分类类别数目，训练了N个svm，每个svm包好了4096个W)，就可以得到结果了。</p>
<p><strong>1.4创新点及缺点</strong><br>
创新点：采用AlexNet CNN网络进行CNN特征提取，为了适应AlexNet网络的输入图像大小，对图片的各种变形方式进行了分析。采用了大样本下的有监督预训练+小样本微调的方式解决小样本难以训练甚至过拟合等问题。<br>
缺点：步骤繁琐，R-CNN的训练先要fine tuning一个预训练的网络，然后针对每个类别都训练一个SVM分类器，最后还要用对bounding-box进行回归，region proposal也要单独用selective search的方式获得。每张图片的每个region proposal都要做卷积，重复操作太多。时间和内存消耗比较大，在训练SVM和回归的时候需要用网络训练的特征作为输入。</p>
<h1 id="2spp-net">2.SPP-net</h1>
<p><strong>2.1概述</strong><br>
在CNN的训练和测试中存在一个技术问题：目前流行的CNN需要一个固定的输入图像大小（例如224*224），这限制了输入图像的纵横比和比例。当应用于任意大小的图像时，当前的方法大多是通过裁剪或扭曲,将输入图像调整到固定大小。但是裁剪区域可能不包含整个对象，而扭曲的内容可能会导致不必要的几何扭曲。由于内容丢失或失真，识别精度可能会受到影响。此外，一个预先定义的尺度当对象比例变化时可能不适用。固定输入大小忽略了涉及规模的问题。<br>
那么为什么cnn需要一个固定的输入大小呢？CNN网络可以分解为卷积网络部分以及全连接网络部分。卷积网络的参数主要是卷积核，完全能够适用任意大小的输入，并且能够产生任意大小的输出。但是全连接层部分不同，全连接层部分的参数是神经元对于所有输入的连接权重，也就是说输入尺寸不固定的话，全连接层参数的个数不能固定。<br>
SPP-net给出的解决方案是，既然只有全连接层需要固定的输入，那么在全连接层前加入一个网络层，使其对任意的输入产生固定的输出即可。SPP-net引入了空间金字塔，其主要思路是对于一副图像分成若干尺度的一些块，然后对于每一块提取特征后融合在一起，这样就可以兼容多个尺度的特征。SPP-net首次将这种思想应用在CNN中，对于卷积层特征也先将其分成不同的尺寸，然后每个尺寸提取一个固定维度的特征，最后拼接这些特征就形成了固定维度的输入。<br>
<img src="https://wangfengyi0228.github.io/post-images/1636700002894.png" alt="" loading="lazy"><br>
从上图可以看出SPP-net和R-CNN的区别，首先是输入不需要放缩或裁剪到指定大小；其次是增加了一个空间金字塔池化层，且每幅图片只需要提取一次特征，这也解决了 R-CNN 重复计算的问题。SPP-net 的思路是由于原图与经过卷积层后得到的特征图在空间位置上存在一定的对应关系，所以只需对整张图像进行一次卷积层特征提取，然后将候选区域在原图的位置映射到卷积层的特征图上得到该候选区域的特征，最后将得到每个候选区域的卷积层特征输入到全连接层进行后续操作。R-CNN 要对每个区域计算卷积，而 SPP-net只需要计算一次卷积，从而节省了大量的计算时间。</p>
<p><strong>2.2网络结构</strong><br>
<img src="https://wangfengyi0228.github.io/post-images/1636700019764.png" alt="" loading="lazy"><br>
空间金字塔池化层的网络结构如上图所示，SPP-net在conv5层得到的特征图是256层，每层都做一次spatial pyramid pooling。先把每个特征图分割成多个不同尺寸的网格，比如网格分别为4<em>4、2</em>2、1<em>1,然后每个网格做max pooling，这样256层特征图就形成了16</em>256，4<em>256，1</em>256维特征，他们连起来就形成了一个固定长度为(4<em>4+2</em>2+1)*256 维的的特征向量，将这个向量输入到后面的全连接层。</p>
<p><strong>2.3优势与不足</strong><br>
SPP-net对R-CNN最大的改进就是特征提取步骤做了修改，特征提取不再需要每个候选区域都经过CNN，只需要将整张图片输入到CNN就可以了，只需要进行一次卷积层特征提取，和R-CNN相比，速度提高了百倍。<br>
然而和RCNN一样，SPP也需要训练CNN提取特征，然后训练SVM分类这些特征。需要巨大的存储空间，并且分开训练或参数微调也很复杂。而且selective search的方法提取特征是在CPU上进行的，相对于GPU来说还是比较慢的。</p>
<h1 id="3总结">3.总结</h1>
<p><strong>(1)R-CNN</strong><br>
R-CNN首次将CNN引入目标检测，其整体流程框架如下图所示：<br>
<img src="https://wangfengyi0228.github.io/post-images/1636700063348.png" alt="" loading="lazy"></p>
<ul>
<li>
<p>优点：<br>
1.为了适应AlexNet网络的输入图像大小，对图片的各种变形方式进行了分析。<br>
2.采用大样本下有监督预训练+小样本微调的方式解决小样本难以训练甚至过拟合等问题（迁移学习）。</p>
</li>
<li>
<p>缺点：<br>
1.针对传统CNN需要固定尺寸的输入图像，crop/warp（归一化）产生物体截断或拉伸，会 导致输入CNN的信息丢失。<br>
2.训练过程太慢，需要对每一个候选区域都输入到cnn中再进行提取特征。<br>
3.步骤相对较多，需要fine-tune预训练模型、训练SVM分类器、用回归器进行精细的调整。<br>
4.占用空间大，提取出的特征以及分类器都需要占用额外的空间。<br>
5.没有对资源进行重复利用，在使用SVM分类和对框进行回归操作的时候cnn模型的参数并没有同步修改。</p>
</li>
</ul>
<p><strong>(2)SPP-Net</strong><br>
SPP-Net的流程与R-CNN类似，只是在提取特征阶段没有对图像进行crop/warp，而是采用空间金字塔池化(Spatial Pyramid Pooling )替换了全连接层之前的最后一个池化层，如下图所示：</p>
<figure data-type="image" tabindex="1"><img src="https://wangfengyi0228.github.io/post-images/1636700158157.png" alt="" loading="lazy"></figure>
<ul>
<li>
<p>优点：<br>
1.取消了crop/warp图像归一化过程，解决图像变形导致的信息丢失以及存储问题。<br>
2.采用空间金字塔池化（SpatialPyramid Pooling ）替换了全连接层之前的最后一个池化层，有效解决了卷积层的重复计算问题。</p>
</li>
<li>
<p>缺点：<br>
1.同R-CNN一样分开训练CNN和SVM、BB回归器，训练SVM的特征需要提前保存在磁盘需要巨大的存储空间；多段训练实现较复杂。<br>
2.CNN和SVM的训练独立导致SVM的训练Loss无法更新SPP-Layer之前的卷积层参数，因此即使采用更深的CNN网络进行特征提取，也无法保证SVM分类器的准确率一定能够提升。<br>
3.使用selective search的方法提取region proposal，是在CPU上进行耗时较长。</p>
</li>
</ul>
]]></content>
    </entry>
</feed>