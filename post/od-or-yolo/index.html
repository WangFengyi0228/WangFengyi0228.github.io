
<!DOCTYPE html>
<html>
  <head>
    
<meta charset="utf-8" >

<title>OD | YOLO | one in a million</title>
<meta name="description" content="我在你的心里，有没有一点特别">

<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.7.0/animate.min.css">

<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.7.2/css/all.css" integrity="sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr" crossorigin="anonymous">
<link rel="shortcut icon" href="https://wangfengyi0228.github.io/favicon.ico?v=1636811837709">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css">
<link rel="stylesheet" href="https://wangfengyi0228.github.io/styles/main.css">



<script src="https://cdn.jsdelivr.net/npm/vue/dist/vue.js"></script>
<script src="https://cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js"></script>



  </head>
  <body>
    <div id="app" class="main">
      <div class="site-header-container">
  <div class="site-header">
    <div class="left">
      <a href="https://wangfengyi0228.github.io">
        <img class="avatar" src="https://wangfengyi0228.github.io/images/avatar.png?v=1636811837709" alt="" width="32px" height="32px">
      </a>
      <a href="https://wangfengyi0228.github.io">
        <h1 class="site-title">one in a million</h1>
      </a>
    </div>
    <div class="right">
      <transition name="fade">
        <i class="icon" :class="{ 'icon-close-outline': menuVisible, 'icon-menu-outline': !menuVisible }" @click="menuVisible = !menuVisible"></i>
      </transition>
    </div>
  </div>
</div>

<transition name="fade">
  <div class="menu-container" style="display: none;" v-show="menuVisible">
    <div class="menu-list">
      
        
          <a href="/" class="menu purple-link">
            首页
          </a>
        
      
        
          <a href="/archives" class="menu purple-link">
            归档
          </a>
        
      
        
          <a href="/tags" class="menu purple-link">
            标签
          </a>
        
      
        
          <a href="/post/about" class="menu purple-link">
            关于
          </a>
        
      
    </div>
  </div>
</transition>


      <div class="content-container">
        <div class="post-detail">
          
            <div class="feature-container" style="background-image: url('https://wangfengyi0228.github.io/post-images/od-or-yolo.jpg')">
            </div>
          
          <h2 class="post-title">OD | YOLO</h2>
          <div class="post-info post-detail-info">
            <span><i class="icon-calendar-outline"></i> 2020-12-28</span>
            
              <span>
                <i class="icon-pricetags-outline"></i>
                
                  <a href="https://wangfengyi0228.github.io/tag/ON6Mm0HVp/">
                    Object Detection
                    
                      ，
                    
                  </a>
                
                  <a href="https://wangfengyi0228.github.io/tag/vYR76uwI3/">
                    Computer Vision
                    
                  </a>
                
              </span>
            
          </div>
          <div class="post-content">
            <p>参考文献</p>
<ul>
<li>[1]Redmon J , Divvala S , Girshick R , et al. You Only Look Once: Unified, Real-Time Object Detection[C]// Computer Vision &amp; Pattern Recognition. IEEE, 2016.</li>
<li>[2]Redmon J , Farhadi A . YOLO9000: Better, Faster, Stronger[C]// IEEE Conference on Computer Vision &amp; Pattern Recognition. IEEE, 2017:6517-6525.</li>
<li>[3]Redmon J , Farhadi A . YOLOv3: An Incremental Improvement[J]. arXiv e-prints, 2018.</li>
<li>[4]Bochkovskiy A , Wang C Y , Liao H Y M . YOLOv4: Optimal Speed and Accuracy of Object Detection[J]. 2020.</li>
</ul>
<h1 id="1yolov1">1.YOLOv1</h1>
<p><strong>1.1 概述</strong><br>
人类的视觉系统快而准确，只要看一眼图像就能知道图像中有什么物体，以及它们之间的相互关系。YOLO（You Only Look Once: Unified, Real-Time Object Detection）正是为了能够进行这样快速、准确的目标检测而诞生的，它创造性地将边界框和类别两个过程合二为一。与基于滑动窗口和region proposal的R-CNN、Fast R-CNN等算法不同，<code>YOLO训练和检测均是在一个单独网络中进行，没有求取region proposal的过程，而是将目标检测作为回归问题求解，基于一个单独的end-to-end网络，完成从原始图像的输入到物体位置和类别的输出。</code><br>
YOLO首先将图片分成<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>S</mi><mo>∗</mo><mi>S</mi></mrow><annotation encoding="application/x-tex">S*S</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.05764em;">S</span></span></span></span>个大小相同的grid，然后让每个grid预测出B个bounding boxes，每个边界框预测一个物体的中心位置(x,y)、高(h)、宽(w)，以及这次预测的置信度，每个grid识别出一种物体。若分类器可以识别C种不同的物体，那么预测结果总共有<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>S</mi><mo>∗</mo><mi>S</mi><mo>∗</mo><mo>(</mo><mi>B</mi><mo>∗</mo><mn>5</mn><mo>+</mo><mi>C</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">S*S*(B*5+C)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">5</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="mclose">)</span></span></span></span>个张量。<br>
<img src="https://wangfengyi0228.github.io/post-images/1636695019326.png" alt="" loading="lazy"><br>
上图展示了图像的分割过程，以及每一个grid预测出的bounding boxes的情况。YOLO的网络结构包括24个卷积层和2个全连接层，其中，卷积层用来提取图像特征，全连接层用来预测图像位置和类别概率值。YOLO网络借鉴了GoogLeNet分类网络结构。不同的是，YOLO未使用inception module，而是使用1x1卷积层（此处1x1卷积层的存在是为了跨通道信息整合）+3x3卷积层简单替代。<br>
<img src="https://wangfengyi0228.github.io/post-images/1636695041225.png" alt="" loading="lazy"><br>
<img src="https://wangfengyi0228.github.io/post-images/1636695047630.png" alt="" loading="lazy"><br>
第一项是物体中心坐标的损失函数，使用的是简单的均方误差；第二项是高和宽的损失函数，使用了平方根的均方误差，是为了使模型对较大的和较小的物体同样敏感，反映出大box中的小偏差比小box中的小偏差更重要。第三项是边框内有对象时的置信度误差，第四项是边框内无对象时的置信度误差，第五项是对象分类误差。<br>
在计算IOU误差时，包含物体的grid与不包含物体的grid，二者的IOU误差对网络loss的贡献值是不同的。两者若采用相同的权值，那么不包含物体的格子的confidence值近似为0，变相放大了包含物体的grid的confidence误差在计算网络参数梯度时的影响，可能会导致模型不稳定。为解决这一问题，YOLO的做法是<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>s</mi><mi>e</mi><mi>t</mi><msub><mi>λ</mi><mrow><mi>c</mi><mi>o</mi><mi>o</mi><mi>r</mi><mi>d</mi></mrow></msub><mo>=</mo><mn>5</mn><mi>a</mi><mi>n</mi><mi>d</mi><msub><mi>λ</mi><mrow><mi>n</mi><mi>o</mi><mi>o</mi><mi>b</mi><mi>j</mi></mrow></msub><mo>=</mo><mn>5</mn></mrow><annotation encoding="application/x-tex">set \lambda _{coord}=5 and \lambda _{noobj}=5</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord mathdefault">s</span><span class="mord mathdefault">e</span><span class="mord mathdefault">t</span><span class="mord"><span class="mord mathdefault">λ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">c</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight" style="margin-right:0.02778em;">r</span><span class="mord mathdefault mtight">d</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.980548em;vertical-align:-0.286108em;"></span><span class="mord">5</span><span class="mord mathdefault">a</span><span class="mord mathdefault">n</span><span class="mord mathdefault">d</span><span class="mord"><span class="mord mathdefault">λ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">n</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">b</span><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">5</span></span></span></span>来进行修正。<br>
<strong>1.2 YOLOv1的优点</strong><br>
YOLO将识别与定位合二为一，结构简洁，检测速度快。相对于R-CNN系列, YOLO的整个流程中都能看到整张图像的信息，因此它在检测物体时能很好的利用上下文信息，从而不容易在背景上预测出错误的物体信息。同时YOLO可以学习到高度泛化的特征，能将一个域上学到的特征迁移到不同但相关的域上，如在自然图像上做训练的YOLO，在艺术图片上可以得到较好的测试结果。</p>
<p><strong>1.3 YOLOv1的缺点</strong><br>
YOLOv1中一个grid只能识别出一种物体，且bounding box的预测准确度不如R-CNN等Region-based方法，其对小物体的检测，尤其是对密集的小物体检测效果较差。</p>
<h1 id="2yolov2">2.YOLOv2</h1>
<p>作者首先在YOLOv1的基础上提出了改进的YOLOv2，然后提出了一种检测与分类联合训练的方法，使用这种联合训练方法在COCO检测数据集和ImageNet分类数据集上训练出了YOLO9000模型，其可以检测超过9000多类物体。<br>
<strong>2.1 YOLOv2的改进策略</strong><br>
YOLOv1虽然检测速度很快，但是在检测精度上却不如R-CNN系检测方法，YOLOv1在物体定位方面不够准确，并且召回率较低。YOLOv2共提出了几种改进策略来提升YOLO模型的定位准确度和召回率，从而提高mAP，YOLOv2在改进中遵循一个原则：保持检测速度，这也是YOLO模型的一大优势。</p>
<ul>
<li>
<p>(1)Batch Normalization<br>
Batch Normalization有助于规范化模型，可以提升模型的收敛速度，而且可以起到一定的正则化效果，降低模型的过拟合。<code>在YOLOv2中，每个卷积层后面都添加了Batch Normalization层，并且不再使用droput。</code>使用Batch Normalization后，YOLOv2的mAP提升了2.4%。</p>
</li>
<li>
<p>(2)High Resolution Classifier<br>
目前大部分的检测模型都会在先在ImageNet分类数据集上预训练模型的主体部分，大多数分类器操作的输入图像小于256×256，分辨率相对较低，不利于检测模型。所以YOLOv1在采用224×224分类模型预训练后，将分辨率增加至448×448。但是直接切换分辨率，检测模型可能难以快速适应高分辨率。<code>所以YOLOv2增加了在ImageNet数据集上使用448×448的分辨率对分类网络进行10个epoch的精细调整，这给了网络时间来调整它的过滤器，以更好地工作在更高分辨率的输出，</code>然后在检测时对对得到的网络进行微调，可以使得模型适用高分辨率输入。使用高分辨率分类器后，YOLOv2的mAP提升了约4%。</p>
</li>
<li>
<p>(3)Convolutional With Anchor Boxes<br>
YOLOv1识别精度低的其中一个原因就是边界框的宽与高的确定存在问题。YOLOv1最后采用的是全连接层直接对边界框进行预测，其中边界框的宽与高是相对整张图片大小的，而由于各个图片中存在不同尺度和长宽比的物体，因此YOLOv1在训练过程中学习适应不同物体的形状是比较困难的，这也导致YOLOv1在精确定位方面表现较差。YOLOv2借鉴了Faster R-CNN中RPN网络的先验框策略。<code>RPN对CNN特征提取器得到的特征图进行卷积来预测每个位置的边界框以及置信度，并且各个位置设置不同尺度和比例的先验框，采用先验框使得模型更容易学习。</code><br>
所以YOLOv2移除了YOLOv1中的全连接层而采用了卷积和anchor boxes来预测边界框。为了使检测所用的特征图分辨率更高，移除其中的一个pool层。在检测模型中，YOLOv2不是采448×448图片作为输入，而是采用416×416大小。因为YOLOv2模型下采样的步长为32，对于416×416大小的图片，最终得到的特征图大小为13×13，维度是奇数，这样特征图恰好只有一个中心位置。对于一些大物体，它们中心点往往落入图片中心位置，此时使用特征图的一个中心点去预测这些物体的边界框相对容易些。<br>
使用anchor boxes之后，YOLOv2的mAP有稍微下降，但召回率有所上升。</p>
</li>
<li>
<p>(4)Dimension Clusters<br>
在Faster R-CNN和SSD中，先验框的长和宽是手动设定的，带有一定的主观性。<code>如果选取的先验框维度比较合适，那么模型更容易学习，从而做出更好的预测。因此，YOLOv2采用k-means聚类方法对训练集中的边界框做了聚类分析。</code>因为设置先验框的主要目的是为了使得预测框与ground truth的IOU更好，所以聚类分析时选用box与聚类中心box之间的IOU值作为距离指标：<br>
<img src="https://wangfengyi0228.github.io/post-images/1636695702119.png" alt="" loading="lazy"></p>
</li>
<li>
<p>(5) New Network：Darknet-19<br>
YOLOv2采用了一个新的基础模型（特征提取器），称为Darknet-19，包括19个卷积层和5个maxpooling层，如Table 6所示。Darknet-19与VGG16模型设计原则是一致的，主要采用3<em>3卷积，采用2</em>2的maxpooling层之后，特征图维度降低2倍，而同时将特征图的channles增加两倍。与NIN(Network in Network)类似，Darknet-19最终采用global avgpooling做预测，并且在3<em>3卷积之间使用1</em>1卷积来压缩特征图channles以降低模型计算量和参数。Darknet-19每个卷积层后面同样使用了batch norm层以加快收敛速度，降低模型过拟合。在ImageNet分类数据集上，Darknet-19的top-1准确度为72.9%，top-5准确度为91.2%，但是模型参数相对小一些。使用Darknet-19之后，YOLOv2的mAP值没有显著提升，但是计算量却可以减少约33%。<br>
<img src="https://wangfengyi0228.github.io/post-images/1636695596885.png" alt="" loading="lazy"></p>
</li>
<li>
<p>(6)Fine-Grained Features<br>
YOLOv2的输入图片大小为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>416</mn><mo>∗</mo><mn>416</mn></mrow><annotation encoding="application/x-tex">416*416</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">4</span><span class="mord">1</span><span class="mord">6</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">4</span><span class="mord">1</span><span class="mord">6</span></span></span></span>，经过5次maxpooling之后得到<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>13</mn><mo>∗</mo><mn>13</mn></mrow><annotation encoding="application/x-tex">13*13</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord">3</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord">3</span></span></span></span>大小的特征图，并以此特征图采用卷积做预测。<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>13</mn><mo>∗</mo><mn>13</mn></mrow><annotation encoding="application/x-tex">13*13</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord">3</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord">3</span></span></span></span>大小的特征图对检测大物体是足够了，但是对于小物体还需要更精细的特征图（Fine-Grained Features）。因此SSD使用了多尺度的特征图来分别检测不同大小的物体，前面更精细的特征图可以用来预测小物体。YOLOv2提出了一种passthrough层来利用更精细的特征图。YOLOv2所利用的Fine-Grained Features是<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>26</mn><mo>∗</mo><mn>26</mn></mrow><annotation encoding="application/x-tex">26*26</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">2</span><span class="mord">6</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">2</span><span class="mord">6</span></span></span></span>大小的特征图（最后一个maxpooling层的输入），对于Darknet-19模型来说就是大小为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>26</mn><mo>∗</mo><mn>26</mn><mo>∗</mo><mn>512</mn></mrow><annotation encoding="application/x-tex">26*26*512</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">2</span><span class="mord">6</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">2</span><span class="mord">6</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">5</span><span class="mord">1</span><span class="mord">2</span></span></span></span>的特征图。passthrough层与ResNet网络的shortcut类似，以前面更高分辨率的特征图为输入，然后将其连接到后面的低分辨率特征图上。前面的特征图维度是后面的特征图的2倍，passthrough层抽取前面层的每个<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>2</mn><mo>∗</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">2*2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">2</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">2</span></span></span></span>的局部区域，然后将其转化为channel维度，对于<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>26</mn><mo>∗</mo><mn>26</mn><mo>∗</mo><mn>512</mn></mrow><annotation encoding="application/x-tex">26*26*512</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">2</span><span class="mord">6</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">2</span><span class="mord">6</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">5</span><span class="mord">1</span><span class="mord">2</span></span></span></span>的特征图，经passthrough层处理之后就变成了<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>13</mn><mo>∗</mo><mn>13</mn><mo>∗</mo><mn>2048</mn></mrow><annotation encoding="application/x-tex">13*13*2048</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord">3</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord">3</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">2</span><span class="mord">0</span><span class="mord">4</span><span class="mord">8</span></span></span></span>的新特征图（特征图大小降低4倍，而channles增加4倍，图6为一个实例），这样就可以与后面的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>13</mn><mo>∗</mo><mn>13</mn><mo>∗</mo><mn>1024</mn></mrow><annotation encoding="application/x-tex">13*13*1024</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord">3</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord">3</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord">0</span><span class="mord">2</span><span class="mord">4</span></span></span></span>特征图连接在一起形成<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>13</mn><mo>∗</mo><mn>13</mn><mo>∗</mo><mn>3072</mn></mrow><annotation encoding="application/x-tex">13*13*3072</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord">3</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord">3</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">3</span><span class="mord">0</span><span class="mord">7</span><span class="mord">2</span></span></span></span>的特征图，然后在此特征图基础上卷积做预测。在YOLO的C源码中，passthrough层称为reorg layer。在TensorFlow中，可以使用tf.extract_image_patches或者tf.space_to_depth来实现passthrough层。<br>
#3.YOLOv3<br>
YOLOv3借鉴了一些好的方案融合到YOLO里面，在保持速度优势的前提下，提升了预测精度，尤其是加强了对小物体的识别能力。</p>
</li>
<li>
<p><strong>DBL：</strong> 代码中的Darknetconv2d_BN_Leaky，是yolo_v3的基本组件,就是卷积+BN+Leaky relu。对于v3来说，BN和leaky relu已经是和卷积层不可分离的部分了(最后一层卷积除外)，共同构成了最小组件。</p>
</li>
<li>
<p>**resn： **n代表数字，有res1，res2, … ,res8等等，表示这个res_block里含有多少个res_unit。这是yolo_v3的大组件，yolo_v3开始借鉴了ResNet的残差结构，使用这种结构可以让网络结构更深(从v2的darknet-19上升到v3的darknet-53，前者没有残差结构)。res_block的基本组件也是DBL。</p>
</li>
<li>
<p>**concat： *<em>张量拼接。将darknet中间层和后面的某一层的上采样进行拼接。拼接的操作和残差层add的操作是不一样的，拼接会扩充张量的维度，而add只是直接相加不会导致张量维度的改变。<br>
<strong>(1)backbone：darknet-53</strong><br>
为了达到更好的分类效果，作者自己设计训练了darknet-53。作者在ImageNet上实验发现这个darknet-53，的确很强，相对于ResNet-152和ResNet-101，darknet-53不仅在分类精度上差不多，计算速度还比ResNet-152和ResNet-101快得多，网络层数也较少。<code>YOLOv3使用了darknet-53的前面的52层（没有全连接层），yolo_v3这个网络是一个全卷积网络，大量使用残差的跳层连接，并且为了降低池化带来的梯度负面效果，作者直接摒弃了Pooling，用conv的stride来实现降采样。</code>在这个网络结构中，使用的是步长为2的卷积来进行降采样。<br>
为了加强算法对小目标检测的精确度，YOLO v3中采用类似FPN的upsample和融合做法（最后融合了3个scale，其他两个scale的大小分别是26×26和52×52），在多个scale的feature map上做检测。<br>
<strong>(2)Output</strong><br>
yolo v3输出了3个不同尺度的feature map，即predictions across scales，这个借鉴了FPN(feature pyramid networks)<code>采用多尺度来对不同size的目标进行检测，越精细的grid cell就可以检测出越精细的物体。</code>3个尺度的feature mapy1,y2和y3的深度都是255，边长的规律是13:26:52。对于COCO类别而言，有80个种类，所以每个box应该对每个种类都输出一个概率。yolo v3设定的是每个网格单元预测3个box，所以每个box需要有(x, y, w, h, confidence)五个基本参数，然后还要有80个类别的概率，3</em>(5 + 80) = 255。而yolo v1的输出张量是7x7x30，只能识别20类物体，显然YOLOv3改进了许多。<br>
<strong>(3)Bounding Box</strong><br>
YOLOv3的Bounding Box由YOLOv2又做出了更好的改进。在YOLOv2和YOLOv3中，都对图像中的object采用了k-means聚类。 feature map中的每一个cell都会预测3个边界框（bounding box），每个bounding box都会预测三个东西：每个框的位置（4个值，中心坐标tx和ty，框的高度bh和宽度bw）、一个objectness prediction 、N个类别，coco数据集80类，voc20类。三次检测，每次对应的感受野不同，适合检测的物体大小不同。<br>
<img src="https://wangfengyi0228.github.io/post-images/1636695931976.png" alt="" loading="lazy"><br>
<strong>(4)LOSS Function</strong><br>
YOLOv3重要改变之一：No more softmaxing the classes。YOLO v3对图像中检测到的对象执行多标签分类。logistic回归用于对anchor包围的部分进行一个目标性评分(objectness score)，即这块位置是目标的可能性有多大。这一步是在predict之前进行的，可以去掉不必要的anchor，减少计算量。不同于faster R-CNN的是，yolo_v3只会对1个prior进行操作，也就是那个最佳prior。而logistic回归就是用来从9个anchor priors中找到objectness score(目标存在可能性得分)最高的那一个。logistic回归就是用曲线对prior相对于 objectness score映射关系的线性建模。</p>
</li>
</ul>
<h1 id="4yolov4">4.YOLOv4</h1>
<p><strong>(1)Bag of freebies</strong><br>
在目标检测中Bag of freebies是指：用一些比较有用的训练技巧来训练模型，从而使目标检测器在不增加推理损耗的情况下达到更好的精度。目标检测经常采用这种方法，并符合这个定义的就是数据增强。<code>数据增强的目的是：增加输入图像的可变性，从而使设计的目标检测模型对不同环境的图片具有较高的鲁棒性。</code><br>
其他的一些Bag of freebies方法是专门解决数据集中的语义分布偏差。在处理语义困扰的问题上，有一个很重要的问题是不同类别之间的数据不平衡，而two-stage 检测器处理这个问题通常是通过hard negative example mining或online hard example mining 。 最后一个bag of freebies是objective function of Bounding Box (BBox) 回归。检测器通常使用MSE对BBOX的中心点和宽高进行回归，至于anchor-based方法，它是为了估算出对应的偏移量。但是，要直接估计BBOX的点坐标值，是要将这些点作为独立变量，但实际上未考虑对象本身的完整性。为了使这一问题得到更好的处理，一些研究人员最近提出的IoU损失，同时考虑预测的BBox面积和ground truth BBox面积覆盖。</p>
<p><strong>(2)Bag of specials</strong><br>
Bag of specials是指一些plugin modules（例如特征增强模型，或者一些后处理），这部分增加的计算量很少，但是能有效地增加物体检测的准确率，将这部分称之为Bag of specials。这部分插件模块能够增强网络模型的一些属性，例如增大感受野（ASFF，ASPP，RFB这些模块）、引入注意力机制（spatial attention和channel attention）、增加特征集成能力。后处理算法是指用一些算法来筛选模型预测出来的结果，后处理方法常用的是NMS，它可以用于过滤那些预测错误的BBoxes，并只保留较高的候选BBoxes。</p>
<p><strong>(3)额外改进</strong><br>
使用一种新的数据增强方法Mosaic和Self-Adversarial Training (SAT)，前者把四张图拼成一张图来训练，变相的等价于增大了mini-batch；后者是在一张图上，让神经网络反向更新图像，对图像做改变扰动，然后在这个图像上训练。<br>
<img src="https://wangfengyi0228.github.io/post-images/1636696843641.png" alt="" loading="lazy"><br>
跨最小批的归一化（Cross mini-batch Normal），在CBN的基础上修改的SAM，从spatial-wise attention修改为point-wise attention：<br>
<img src="https://wangfengyi0228.github.io/post-images/1636696855738.png" alt="" loading="lazy"><br>
修改的PAN，把通道从相加（add）改变为concat：<br>
<img src="https://wangfengyi0228.github.io/post-images/1636696863193.png" alt="" loading="lazy"><br>
<strong>(4)YOLOv4的组成</strong><br>
Backbone: CSPDarknet53<br>
Neck: SPP , PAN<br>
Head: YOLOv3</p>
<h1 id="5总结">5.总结</h1>
<p><strong>(1)YOLOv1</strong></p>
<ul>
<li>训练和检测均是在一个单独网络中进行，没有求取region proposal的过程。</li>
<li>将目标检测作为回归问题求解，基于一个单独的end-to-end网络。</li>
<li>损失函数中的几个项与输出向量中的内容相对应。</li>
<li>YOLOv1检测速度快，但在检测小物体以及密集物体方面存在欠缺。<br>
<strong>(2)YOLOv2</strong></li>
<li>YOLOv2在保证检测速度的基础上提出了几种改进策略来提升YOLO模型的定位准确度和召回率。</li>
<li>网络结构中每个卷积层后面都添加了Batch Normalization层，并且使用高了分辨率分类器。</li>
<li>YOLOv2采用k-means聚类方法对训练集中的边界框做了聚类分析，使得选取的先验框维度更合适，模型更容易学习，从而做出更好的预测。<br>
<strong>(3)YOLOv3</strong></li>
<li>YOLOv3借鉴了一些好的方案融合到YOLO里面，在保持速度优势的前提下，提升了预测精度，尤其是加强了对小物体的识别能力。</li>
<li>Darknetconv2d_BN_Leaky是yolo_v3的基本组件,就是卷积+BN+Leaky relu。</li>
<li>YOLOv3开始借鉴了ResNet的残差结构，使用这种结构可以让网络结构更深。</li>
<li>yolo v3输出了3个不同尺度的feature map，采用多尺度来对不同size的目标进行检测，越精细的grid cell就可以检测出越精细的物体。</li>
<li>YOLOv3的Bounding Box由YOLOv2又做出了更好的改进。在YOLOv2和YOLOv3中，都对图像中的object采用了k-means聚类。 feature map中的每一个cell都会预测3个边界框。三次检测，每次对应的感受野不同，适合检测的物体大小不同。<br>
<strong>(4)YOLOv4</strong></li>
<li>YOLOv4研究设计了一个简单且高效的目标检测算法，该算法降低了训练门槛，使得普通人员在拥有一块1080TI或者2080TI的情况下就能够训练一个super fast and accurate 的目标检测器。</li>
<li>在训练过程中，验证了最新的Bag-of-Freebies和Bag-of-Specials对Yolo-V4的影响。</li>
<li>简化以及优化了一些最新提出的算法，包括（CBN，PAN，SAM），从而使Yolo-V4能够在一块GPU上就可以训练起来。</li>
</ul>
<p>YOLOv5初步移植到手机后的效果图：<br>
<img src="https://wangfengyi0228.github.io/post-images/1636696976292.png" alt="" loading="lazy"></p>

          </div>
        </div>

        
          <div class="next-post">
            <a class="purple-link" href="https://wangfengyi0228.github.io/post/movies-or-call-me-by-your-name/">
              <h3 class="post-title">
                下一篇：Movies | Call Me By Your Name
              </h3>
            </a>
          </div>
          
      </div>

      

      <div class="site-footer">
  <div class="slogan">我在你的心里，有没有一点特别</div>
  <div class="social-container">
    
      
    
      
    
      
    
      
    
      
    
  </div>
  Copyright by WFY | <a class="rss" href="https://wangfengyi0228.github.io/atom.xml" target="_blank">RSS</a>
<div>
	<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
	<span id="busuanzi_container_site_uv">总访客人数量<span id="busuanzi_value_site_uv"></span>人次</span>
	<span id="busuanzi_container_site_pv">本站总访问量<span id="busuanzi_value_site_pv"></span>次</span>
          </div>
</div>


    </div>
    <script type="application/javascript">

hljs.initHighlightingOnLoad()

var app = new Vue({
  el: '#app',
  data: {
    menuVisible: false,
  },
})

</script>




  </body>
</html>
